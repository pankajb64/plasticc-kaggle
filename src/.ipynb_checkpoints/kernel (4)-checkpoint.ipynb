{
  "cells": [
    {
      "metadata": {
        "_uuid": "4aec2f48f28631a15d7fcd976b533a4b42217fe2"
      },
      "cell_type": "markdown",
      "source": "**UPDATE:**\n\nI made a couple of changes since submitting my first set of predictions, which include scaling the flux observations and doing per-channel convolutions. Together, these two tweaks really improved my model, and ** halfed the test loss from 24 to 12**. I've shared the updates in Bold below (Also updated the TOC headers)"
    },
    {
      "metadata": {
        "_uuid": "fa00d65a0e757b6e8531e5781ee40c2b1a3c3853"
      },
      "cell_type": "markdown",
      "source": "Hey guys!\nThis is my first real kernel for a Kaggle competition, so feedback is greatly appreciated!"
    },
    {
      "metadata": {
        "_uuid": "dbfddbc5096b829222f7b548fa75b1aec455eb6f"
      },
      "cell_type": "markdown",
      "source": "# Table of Contents\n\n- [Introduction](#Introduction)\n- [Approach](#Approach)\n- [Implementation](#Implementation)\n- [DMDT Images](#DMDT-Images)\n    - [Generating the Images **UPDATED**](#Generating-the-Images)\n    - [Loading the Images](#Loading-the-Images)\n- [Training and Test set](#Training-and-Test-set)\n- [The CNN Model Architecture **UPDATED**](#The-CNN-Model-Architecture)\n- [Custom Loss Function](#Custom-Loss-Function)\n- [Training and Evaluation](#Training-and-Evaluation)"
    },
    {
      "metadata": {
        "_uuid": "1d3c8aa637c3872a26b87273b583cde465fd5461"
      },
      "cell_type": "markdown",
      "source": "# Introduction\n\nA little bit about myself - by day, I'm just a regular software engineer, trying to eek my way into data science. But by night, I'm a huge astrophysics/cosmology buff. I try to keep up with what's happening by reading news, watching videos, and even going through some lectures as well as reading a few papers. So, this competition was literally the icing on the cake for me! It was fascinating to go through the two example kernels - to learn about the [motivation and the astronomical background](https://www.kaggle.com/michaelapers/the-plasticc-astronomy-starter-kit) (super fun stuff!) as well to look at a [sample approach taken by astronomers](https://www.kaggle.com/michaelapers/the-plasticc-astronomy-classification-demo). \n\nIt was equally (if not more) fun to look at the different kernels posted by everyone here. I'm yet to go through a large chunk of them, so its possible my approach has already been shared by someone, but in general I found some of these particularly interesting/useful - \n\n- [The Astronomical (complete) EDA - PLAsTiCC dataset](https://www.kaggle.com/danilodiogo/the-astronomical-complete-eda-plasticc-dataset) - this does a better job at EDA than I could've done.\n- [All Classes Light Curve Characteristics](https://www.kaggle.com/mithrillion/all-classes-light-curve-characteristics)\n- [Using CNNs in time series analysis](https://www.kaggle.com/hrmello/using-cnns-in-time-series-analysis) - this one in particular since my approach was inspired by it.\n\nI did look at a few basic options such Random Forests/XGBoost, Logistic Regression, etc. But the CNN-based approach seemed to interest me, given that I have some prior experience working with CNNs. Plus, the fact that I sort of started to get into it right away. A short google search yielded a paper that uses CNNs to classify light curves, which I describe below. \n"
    },
    {
      "metadata": {
        "_uuid": "8b6332fc2c0b9773be791d8396c00e61768699fc"
      },
      "cell_type": "markdown",
      "source": "# Approach\n\nBased on [Mahabal, Ashish, et al. \"Deep-learnt classification of light curves.\" ](https://arxiv.org/pdf/1709.06257.pdf)\n\n\nI came across this paper and I liked it because it directly tackles the problem at hand - classifying light curves based on time series data is a hard problem. Firstly, because there is often an irregular gap in the observations due to a variety of reasons. Secondly, and this is especially true with LSST, there is always the scope of coming across objects that you've never seen before (hence the training set isn't a good representation of the test set). This makes computing features challenging. Generic statistical features don't yield high accuracy. Sometimes, domain-level features are employed (the paper gives an example feature - 'fading profile of a single peaked fast transient'. These features are useful only for certain classes of objects and don't generalize well.\n\nThe authors present a different approach - to use neural networks to classify objects, since neural networks are good at extracting features from data. In particular, they use Convolutional Neural Networks, which have proven their metal in image classification. You can read more about CNNs [here](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/). \n\nIn short, the idea is to encode the flux values into a matrix (or a tensor), and use that as input to a CNN (which I implement using Keras). \n\nThe encoding approach is interesting, the idea is to capture changes in flux values at different magnitude and time scales. For each pair of points on the light curve, compute the difference in magnitude $dm$ and time $dt$ and then put them into bins. Each bin contains counts of all pairs of points that fall within it.  The resulting matrix/image is referred in the paper as a _dmdt_ image. There are $p = {n \\choose 2}$ such pairs for n points on the light curve. The sample bins used in the paper (as well my code) are \n$$dm = \\pm [0, 0.1, 0.2, 0.3, 0.5, 1, 1.5, 2, 2.5, 3, 5, 8]$$ \nand \n$$dt = [\\frac{1}{145}, \\frac{2}{145}, \\frac{3}{145}, \\frac{4}{145}, \\frac{1}{25}, \\frac{2}{25}, \\frac{3}{25}, 1.5, 2.5, 3.5, 4.5, 5.5, 7, 10, 20, 30, 60, 90, 120, 240, 600, 960, 2000, 4000]$$\n\nThe bins are in approximately in a semi-logarithmic fashion, so that smaller changes in magnitude and time are spread over many bins, whereas the corresponding larger changes are clubbed together. The flux counts are normalized and stretched to fit between 0 and 255 - $${norm}_{bin} = \\lfloor{\\frac{255 * {count}_{bin}}{p} + 0.99999}\\rfloor$$\n\nI also tried to create a kernel just to visualize these images and do some basic EDA stuff, but I haven't got around to finishing that, you can find the kernel here - [Plasticc DMDT EDA](https://www.kaggle.com/pankajb64/plasticc-dmdt-eda)\n\nThe authors talk about a single light curve, and hence creating a single 2D matrix, but in our case we have 6 different passbands, so 6 such matrices per object. The simplest approach, which is also the one that I use here, is to simply stack these images to create a 6-channel image and feed that to the CNN. Its not ideal, since different bands are measured at different time instants, and the total number of measurements vary across bands. On the other hand, treating each band as an individual input to the CNN isn't a great idea either (though I haven't tried it so can't be sure) since the different bands combine to uniquely identify the objects, and each band may potentially contain different information. Its not clear to me what the best way to deal with this, so feedback is greatly appreciated.\n\nI also implemented the weighted multi-class log loss used in this competition as a custom loss function, since I wasn't sure categorical cross-entropy takes class imbalance into account. In my case I set the weight of each class to be 1, since I obviously don't know the actual weights used by the compeition judges.\n\nSo without further ado, let's just get to the code!"
    },
    {
      "metadata": {
        "_uuid": "6befde97937c38ef7410363490a89dd0c788745b"
      },
      "cell_type": "markdown",
      "source": "# Implementation"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport pickle\nimport multiprocessing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras.backend as K #to define custom loss function\nimport tensorflow as tf #We'll use tensorflow backend here\nimport dask.dataframe as dd\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tnrange, tqdm_notebook\nfrom collections import OrderedDict\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Input, AveragePooling1D, Reshape, DepthwiseConv2D, SeparableConv2D\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom datetime import datetime\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\n\nprint(os.listdir(\"../input/\"))",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['PLAsTiCC-2018', 'plasticc_dmdt_images']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "daecdc4e92a586d95cd3fad7a738ac52b7cd8037",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('../input/PLAsTiCC-2018/training_set.csv')",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6859aad5643c5fb69c054604c1cfd82b6b303e46"
      },
      "cell_type": "code",
      "source": "df.head(5)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "   object_id         mjd  passband        flux   flux_err  detected\n0        615  59750.4229         2 -544.810303   3.622952         1\n1        615  59750.4306         1 -816.434326   5.553370         1\n2        615  59750.4383         3 -471.385529   3.801213         1\n3        615  59750.4450         4 -388.984985  11.395031         1\n4        615  59752.4070         2 -681.858887   4.041204         1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_id</th>\n      <th>mjd</th>\n      <th>passband</th>\n      <th>flux</th>\n      <th>flux_err</th>\n      <th>detected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>615</td>\n      <td>59750.4229</td>\n      <td>2</td>\n      <td>-544.810303</td>\n      <td>3.622952</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>615</td>\n      <td>59750.4306</td>\n      <td>1</td>\n      <td>-816.434326</td>\n      <td>5.553370</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>615</td>\n      <td>59750.4383</td>\n      <td>3</td>\n      <td>-471.385529</td>\n      <td>3.801213</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>615</td>\n      <td>59750.4450</td>\n      <td>4</td>\n      <td>-388.984985</td>\n      <td>11.395031</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>615</td>\n      <td>59752.4070</td>\n      <td>2</td>\n      <td>-681.858887</td>\n      <td>4.041204</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0fbf0439de22ec0595e88f252f320be267a5ed31"
      },
      "cell_type": "markdown",
      "source": "## DMDT Images"
    },
    {
      "metadata": {
        "_uuid": "6f7846292dffc73b95fb2a7b94aff16a51b35f71"
      },
      "cell_type": "markdown",
      "source": "### Generating the Images\n\nAs mentioned earlier, in this kernel, I'm not doing EDA, but just getting straight to how to generate the dmdt images, and run the model using them.\n\nThe functions below generate dmdt images (or \"dmdtize\" the input as I say). I decided to get around the large number of rows by saving the image for each object in a different file, and combining them while loading. That way, I didn't have to re-process images if I happened to stop the run mid-way.\n\n**UPDATE**:\n\nDuring my post-submission analysis, I got a chance to re-evaluate a larger set of dmdt images, and I realized a bulk of them looked very similar even though they were from different classes. So I decided to scale the observations for each individual object to be in the range defined by the flux magnitude bins, i.e. between -8 and 8."
    },
    {
      "metadata": {
        "_uuid": "3abc3464dd70912c94866219a5c568afa7084da2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def dmdtize_single_band(df, dm_bins, dt_bins, col):\n    n_points = df.shape[0]\n    dmdt_img = np.zeros((len(dm_bins), len(dt_bins)), dtype='int')\n    for i in range(n_points):\n        for j in range(i+1, n_points):\n            dmi = df.iloc[i][col]\n            dmj = df.iloc[j][col]\n            dti = df.iloc[i]['mjd']\n            dtj = df.iloc[j]['mjd']\n            \n            dm = dmj - dmi if dtj > dti else dmi - dmj\n            dt = abs(dtj - dti)\n            \n            dm_idx = min(np.searchsorted(dm_bins, dm), len(dm_bins)-1)\n            dt_idx = min(np.searchsorted(dt_bins, dt), len(dt_bins)-1)\n            \n            dmdt_img[dm_idx, dt_idx] += 1\n    return dmdt_img",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f084d72fd167ed204c267d55c3eb198e5e95537",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def dmdtize_single_object(args):\n    (df, object_id, base_dir) = args\n    key = '{}/{}_dmdt.pkl'.format(base_dir, object_id)\n    if os.path.isfile(key):\n        return\n    num_bands = 6\n    dm_bins = [-8, -5, -3, -2.5, -2, -1.5, -1, -0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5, 1, 1.5, 2, 2.5, 3, 5, 8]\n    dt_bins = [1/145, 2/145, 3/145, 4/145, 1/25, 2/25, 3/25, 1.5, 2.5, 3.5, 4.5, 5.5, 7, 10, 20, 30, 60, 90, 120, 240, 600, 960, 2000, 4000]\n    dmdt_img = np.zeros((len(dm_bins), len(dt_bins), num_bands), dtype='int')\n    \n    mms = MinMaxScaler(feature_range=(-8, 8))\n    df['local_tr_flux'] = mms.fit_transform(df['flux'].values.reshape(-1,1))\n    \n    max_points = 0\n    for band_idx in range(num_bands):\n        df_band = df.loc[df['passband'] == band_idx]\n        dmdt_img[:, :, band_idx] = dmdtize_single_band(df_band, dm_bins, dt_bins, 'local_tr_flux')\n        if band_idx == 0 or df_band.shape[0] > max_points:\n            max_points = df_band.shape[0] #store max points to scale the image later\n    \n    max_pairs = (max_points*(max_points-1))//2\n    dmdt_img = np.floor(255*dmdt_img/max_pairs + 0.99999).astype('int')\n    with open(key, 'wb') as f:\n        pickle.dump(dmdt_img, f)        ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6d96b536eb7bee7035e1424bb2811e0a94e7fc13",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def dmdtize(df, base_dir='train'):\n    objects = df['object_id'].drop_duplicates().values\n    nobjects = len(objects)\n    dmdt_img_dict = {}\n    pool = multiprocessing.Pool()\n    df_args = []\n    for obj in objects:\n        df_obj = df.loc[df['object_id'] == obj]\n        df_args.append((df_obj, obj, base_dir))\n    pool.map(dmdtize_single_object, df_args)\n    pool.terminate()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b5cfd70db611deda57a5becc5cabc97e2d1f440d"
      },
      "cell_type": "markdown",
      "source": "To generate/save the dmdt images, convert thes cell below to code and execute. It takes a while (it took me about 20 minutes on a 16-core machine, using all cores), so I've attached the pre-processed images as a dataset, so they can be loaded up easily."
    },
    {
      "metadata": {
        "_uuid": "4e8f9597f7f7933928e327a0a738025d9b055a1f"
      },
      "cell_type": "markdown",
      "source": "```\ndmdtize(df)\n```"
    },
    {
      "metadata": {
        "_uuid": "0b035a6fa7f5b91ce66aedfdb2c54a118840ed54"
      },
      "cell_type": "markdown",
      "source": "### Loading the Images\n\nThe cells below load the dmdt images from the dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "634bc3f85159742502089874717862d2eaeef6f6"
      },
      "cell_type": "code",
      "source": "objects = df['object_id'].drop_duplicates().values",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4966d5dad48ee6336c054543a5c948a047dac9c4"
      },
      "cell_type": "code",
      "source": "def load_dmdt_images(objects, base_dir='train'):\n    dmdt_img_dict = OrderedDict()\n    for obj in objects:\n        key = '{}/{}_dmdt.pkl'.format(base_dir, obj)\n        if os.path.isfile(key):\n            with(open(key, 'rb')) as f:\n                dmdt_img_dict[obj] = pickle.load(f)\n    return dmdt_img_dict",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d33903aa9215f1828cf856f803d18bccb189a548"
      },
      "cell_type": "code",
      "source": "dmdt_img_dict = load_dmdt_images(objects, '../input/plasticc_dmdt_images/train/data1/plasticc/input/train')",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b5cf88b3e2cd7348157846d25b62dc1671dbd111"
      },
      "cell_type": "markdown",
      "source": "## Training and Test set\n\nThe images are already scaled and pre-processed, so we can just feed them to the CNN. Lets create the input and output vectors for training and test."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df5cce2d431a1220ab8dc12ec232600bef453a43"
      },
      "cell_type": "code",
      "source": "X = np.array(list(dmdt_img_dict.values()), dtype='int')\n\ndf_meta = pd.read_csv('../input/PLAsTiCC-2018/training_set_metadata.csv')\nlabels = pd.get_dummies(df_meta.loc[df_meta['object_id'].isin(dmdt_img_dict.keys()) , 'target'])\n\ny = labels.values",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0befe3bb0d724329a6a0c894b96a44ce15e999d"
      },
      "cell_type": "code",
      "source": "df_meta",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "      object_id          ra       decl   ...    distmod  mwebv  target\n0           615  349.046051 -61.943836   ...        NaN  0.017      92\n1           713   53.085938 -27.784405   ...    45.4063  0.007      88\n2           730   33.574219  -6.579593   ...    40.2561  0.021      42\n3           745    0.189873 -45.586655   ...    40.7951  0.007      90\n4          1124  352.711273 -63.823658   ...    40.4166  0.024      90\n5          1227   35.683594  -5.379379   ...        NaN  0.020      65\n6          1598  347.846710 -64.760857   ...    39.7279  0.019      90\n7          1632  348.595886 -63.072620   ...    43.1524  0.021      42\n8          1920  149.414062   3.433834   ...    41.1401  0.027      90\n9          1926  149.414062   1.940072   ...        NaN  0.018      65\n10         2072    0.965665 -46.375080   ...    39.8317  0.007      90\n11         2103  346.500000 -62.320400   ...    42.4667  0.020      42\n12         2300  359.446716 -44.201530   ...    46.7959  0.010      42\n13         2330  359.805206 -46.768478   ...    42.6207  0.011      90\n14         2624  346.655182 -63.260487   ...        NaN  0.019      65\n15         2677   53.964844 -28.630989   ...        NaN  0.009      16\n16         2922  352.398651 -62.696659   ...    39.2171  0.020      67\n17         3041  346.130127 -63.072620   ...    38.8800  0.020      67\n18         3285  150.820312   1.641510   ...    39.7258  0.020      42\n19         3423  349.615387 -63.636005   ...    44.4078  0.018      95\n20         3489  150.117188   2.836105   ...    45.0753  0.016      88\n21         3910    0.589520 -47.161343   ...    46.7274  0.009      62\n22         4088    0.965665 -46.375080   ...    42.0691  0.007      88\n23         4132  359.811707 -45.191612   ...    36.9750  0.010      42\n24         4171    2.097458 -45.783966   ...        NaN  0.011      16\n25         4173  152.050781   3.284369   ...    42.5158  0.019      15\n26         4220  358.648071 -46.375080   ...    38.9679  0.009      42\n27         4389  151.699219   3.583322   ...    40.1939  0.016      90\n28         4595  349.615387 -63.636005   ...    42.7370  0.018      90\n29         4819   35.332031  -5.979157   ...    40.8445  0.022      90\n...         ...         ...        ...   ...        ...    ...     ...\n7818  130219752  355.163544 -50.091457   ...        NaN  0.013       6\n7819  130231675   22.851562 -18.681826   ...    39.5014  0.014      42\n7820  130263372   86.132812 -13.708317   ...    40.9681  0.187      90\n7821  130312781  133.242188 -24.788561   ...        NaN  0.131      16\n7822  130319749   11.074219  -8.084014   ...    37.4496  0.028      42\n7823  130330088  340.839844 -32.442867   ...        NaN  0.010      65\n7824  130359176  138.164062 -40.033035   ...    37.7087  0.417      52\n7825  130375489  356.132812 -27.111860   ...    40.9044  0.016      90\n7826  130386135   28.828125 -25.117701   ...        NaN  0.011      65\n7827  130402542  140.976562   3.732834   ...    39.6994  0.032      15\n7828  130408188  314.296875 -40.033035   ...        NaN  0.036      16\n7829  130414189   36.480583 -51.642994   ...    37.8452  0.024      90\n7830  130489916  209.707031 -38.300922   ...        NaN  0.074      16\n7831  130552230   54.316406 -18.524391   ...    39.1698  0.061      42\n7832  130595291  304.980469 -14.169522   ...    39.9839  0.061      15\n7833  130617044   68.378906 -16.334824   ...    39.4875  0.035      52\n7834  130622528  132.539062  -1.641510   ...        NaN  0.015      65\n7835  130639669  164.003906 -31.039240   ...    40.8250  0.070      62\n7836  130659834  174.902344  -2.089372   ...    41.5364  0.024      42\n7837  130678775  142.734375 -23.480536   ...    38.0319  0.056      64\n7838  130684460  117.949219  -0.895283   ...        NaN  0.053      16\n7839  130695262   90.000000 -20.264481   ...    46.6624  0.043      15\n7840  130698059  142.734375 -18.997131   ...    37.0254  0.045      67\n7841  130716752  337.372894 -57.209084   ...        NaN  0.016      16\n7842  130727624  294.960938  -4.031936   ...        NaN  0.291      65\n7843  130739978   26.718750 -14.940303   ...        NaN  0.013      65\n7844  130755807  120.101349 -62.696659   ...    46.6108  0.136      90\n7845  130762946  203.108109 -55.682144   ...        NaN  0.430      16\n7846  130772921   79.101562 -35.501846   ...        NaN  0.034      65\n7847  130779836  301.992188 -17.426323   ...        NaN  0.091       6\n\n[7848 rows x 12 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_id</th>\n      <th>ra</th>\n      <th>decl</th>\n      <th>gal_l</th>\n      <th>gal_b</th>\n      <th>ddf</th>\n      <th>hostgal_specz</th>\n      <th>hostgal_photoz</th>\n      <th>hostgal_photoz_err</th>\n      <th>distmod</th>\n      <th>mwebv</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>615</td>\n      <td>349.046051</td>\n      <td>-61.943836</td>\n      <td>320.796530</td>\n      <td>-51.753706</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.017</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>713</td>\n      <td>53.085938</td>\n      <td>-27.784405</td>\n      <td>223.525509</td>\n      <td>-54.460748</td>\n      <td>1</td>\n      <td>1.8181</td>\n      <td>1.6267</td>\n      <td>0.2552</td>\n      <td>45.4063</td>\n      <td>0.007</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>730</td>\n      <td>33.574219</td>\n      <td>-6.579593</td>\n      <td>170.455585</td>\n      <td>-61.548219</td>\n      <td>1</td>\n      <td>0.2320</td>\n      <td>0.2262</td>\n      <td>0.0157</td>\n      <td>40.2561</td>\n      <td>0.021</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>745</td>\n      <td>0.189873</td>\n      <td>-45.586655</td>\n      <td>328.254458</td>\n      <td>-68.969298</td>\n      <td>1</td>\n      <td>0.3037</td>\n      <td>0.2813</td>\n      <td>1.1523</td>\n      <td>40.7951</td>\n      <td>0.007</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1124</td>\n      <td>352.711273</td>\n      <td>-63.823658</td>\n      <td>316.922299</td>\n      <td>-51.059403</td>\n      <td>1</td>\n      <td>0.1934</td>\n      <td>0.2415</td>\n      <td>0.0176</td>\n      <td>40.4166</td>\n      <td>0.024</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1227</td>\n      <td>35.683594</td>\n      <td>-5.379379</td>\n      <td>171.992947</td>\n      <td>-59.253501</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.020</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1598</td>\n      <td>347.846710</td>\n      <td>-64.760857</td>\n      <td>318.929827</td>\n      <td>-49.143596</td>\n      <td>1</td>\n      <td>0.1352</td>\n      <td>0.1820</td>\n      <td>0.0304</td>\n      <td>39.7279</td>\n      <td>0.019</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1632</td>\n      <td>348.595886</td>\n      <td>-63.072620</td>\n      <td>320.023289</td>\n      <td>-50.713060</td>\n      <td>1</td>\n      <td>0.6857</td>\n      <td>0.7014</td>\n      <td>0.0100</td>\n      <td>43.1524</td>\n      <td>0.021</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1920</td>\n      <td>149.414062</td>\n      <td>3.433834</td>\n      <td>234.919132</td>\n      <td>42.245550</td>\n      <td>1</td>\n      <td>0.3088</td>\n      <td>0.3229</td>\n      <td>0.3360</td>\n      <td>41.1401</td>\n      <td>0.027</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1926</td>\n      <td>149.414062</td>\n      <td>1.940072</td>\n      <td>236.565366</td>\n      <td>41.393323</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.018</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2072</td>\n      <td>0.965665</td>\n      <td>-46.375080</td>\n      <td>325.845907</td>\n      <td>-68.579427</td>\n      <td>1</td>\n      <td>0.1516</td>\n      <td>0.1900</td>\n      <td>0.0104</td>\n      <td>39.8317</td>\n      <td>0.007</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2103</td>\n      <td>346.500000</td>\n      <td>-62.320400</td>\n      <td>321.951129</td>\n      <td>-50.736054</td>\n      <td>1</td>\n      <td>0.1695</td>\n      <td>0.5409</td>\n      <td>0.2283</td>\n      <td>42.4667</td>\n      <td>0.020</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2300</td>\n      <td>359.446716</td>\n      <td>-44.201530</td>\n      <td>331.730015</td>\n      <td>-69.805709</td>\n      <td>1</td>\n      <td>0.2360</td>\n      <td>2.7474</td>\n      <td>0.5335</td>\n      <td>46.7959</td>\n      <td>0.010</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2330</td>\n      <td>359.805206</td>\n      <td>-46.768478</td>\n      <td>327.135979</td>\n      <td>-67.829903</td>\n      <td>1</td>\n      <td>0.4541</td>\n      <td>0.5736</td>\n      <td>0.2827</td>\n      <td>42.6207</td>\n      <td>0.011</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2624</td>\n      <td>346.655182</td>\n      <td>-63.260487</td>\n      <td>320.952196</td>\n      <td>-50.040935</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.019</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2677</td>\n      <td>53.964844</td>\n      <td>-28.630989</td>\n      <td>225.142950</td>\n      <td>-53.813613</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.009</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2922</td>\n      <td>352.398651</td>\n      <td>-62.696659</td>\n      <td>318.017427</td>\n      <td>-51.967966</td>\n      <td>1</td>\n      <td>0.1539</td>\n      <td>0.1469</td>\n      <td>0.0094</td>\n      <td>39.2171</td>\n      <td>0.020</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>3041</td>\n      <td>346.130127</td>\n      <td>-63.072620</td>\n      <td>321.423103</td>\n      <td>-50.042305</td>\n      <td>1</td>\n      <td>0.1069</td>\n      <td>0.1274</td>\n      <td>0.0198</td>\n      <td>38.8800</td>\n      <td>0.020</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3285</td>\n      <td>150.820312</td>\n      <td>1.641510</td>\n      <td>237.994507</td>\n      <td>42.358984</td>\n      <td>1</td>\n      <td>0.1610</td>\n      <td>0.1818</td>\n      <td>0.0079</td>\n      <td>39.7258</td>\n      <td>0.020</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3423</td>\n      <td>349.615387</td>\n      <td>-63.636005</td>\n      <td>318.927246</td>\n      <td>-50.506542</td>\n      <td>1</td>\n      <td>1.9876</td>\n      <td>1.1213</td>\n      <td>0.1591</td>\n      <td>44.4078</td>\n      <td>0.018</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3489</td>\n      <td>150.117188</td>\n      <td>2.836105</td>\n      <td>236.124718</td>\n      <td>42.483719</td>\n      <td>1</td>\n      <td>1.1330</td>\n      <td>1.4377</td>\n      <td>0.2168</td>\n      <td>45.0753</td>\n      <td>0.016</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>3910</td>\n      <td>0.589520</td>\n      <td>-47.161343</td>\n      <td>325.385896</td>\n      <td>-67.769893</td>\n      <td>1</td>\n      <td>0.1969</td>\n      <td>2.6766</td>\n      <td>0.5926</td>\n      <td>46.7274</td>\n      <td>0.009</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>4088</td>\n      <td>0.965665</td>\n      <td>-46.375080</td>\n      <td>325.845907</td>\n      <td>-68.579427</td>\n      <td>1</td>\n      <td>0.4833</td>\n      <td>0.4644</td>\n      <td>0.0321</td>\n      <td>42.0691</td>\n      <td>0.007</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>4132</td>\n      <td>359.811707</td>\n      <td>-45.191612</td>\n      <td>329.485675</td>\n      <td>-69.150905</td>\n      <td>1</td>\n      <td>0.0561</td>\n      <td>0.0556</td>\n      <td>0.0301</td>\n      <td>36.9750</td>\n      <td>0.010</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>4171</td>\n      <td>2.097458</td>\n      <td>-45.783966</td>\n      <td>324.737840</td>\n      <td>-69.478613</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.011</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>4173</td>\n      <td>152.050781</td>\n      <td>3.284369</td>\n      <td>237.157374</td>\n      <td>44.318466</td>\n      <td>1</td>\n      <td>0.5149</td>\n      <td>0.5512</td>\n      <td>0.0221</td>\n      <td>42.5158</td>\n      <td>0.019</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>4220</td>\n      <td>358.648071</td>\n      <td>-46.375080</td>\n      <td>329.462659</td>\n      <td>-67.716008</td>\n      <td>1</td>\n      <td>0.1197</td>\n      <td>0.1322</td>\n      <td>0.3351</td>\n      <td>38.9679</td>\n      <td>0.009</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>4389</td>\n      <td>151.699219</td>\n      <td>3.583322</td>\n      <td>236.533224</td>\n      <td>44.205648</td>\n      <td>1</td>\n      <td>0.2333</td>\n      <td>0.2205</td>\n      <td>0.9667</td>\n      <td>40.1939</td>\n      <td>0.016</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>4595</td>\n      <td>349.615387</td>\n      <td>-63.636005</td>\n      <td>318.927246</td>\n      <td>-50.506542</td>\n      <td>1</td>\n      <td>0.5919</td>\n      <td>0.5995</td>\n      <td>0.0127</td>\n      <td>42.7370</td>\n      <td>0.018</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>4819</td>\n      <td>35.332031</td>\n      <td>-5.979157</td>\n      <td>172.286722</td>\n      <td>-59.931743</td>\n      <td>1</td>\n      <td>0.3053</td>\n      <td>0.2870</td>\n      <td>0.0076</td>\n      <td>40.8445</td>\n      <td>0.022</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7818</th>\n      <td>130219752</td>\n      <td>355.163544</td>\n      <td>-50.091457</td>\n      <td>328.699808</td>\n      <td>-63.346833</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.013</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7819</th>\n      <td>130231675</td>\n      <td>22.851562</td>\n      <td>-18.681826</td>\n      <td>172.442571</td>\n      <td>-77.518364</td>\n      <td>0</td>\n      <td>0.0949</td>\n      <td>0.1656</td>\n      <td>0.0257</td>\n      <td>39.5014</td>\n      <td>0.014</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7820</th>\n      <td>130263372</td>\n      <td>86.132812</td>\n      <td>-13.708317</td>\n      <td>218.054844</td>\n      <td>-20.910333</td>\n      <td>0</td>\n      <td>0.2456</td>\n      <td>0.3015</td>\n      <td>0.0576</td>\n      <td>40.9681</td>\n      <td>0.187</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>7821</th>\n      <td>130312781</td>\n      <td>133.242188</td>\n      <td>-24.788561</td>\n      <td>249.573365</td>\n      <td>12.560887</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.131</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7822</th>\n      <td>130319749</td>\n      <td>11.074219</td>\n      <td>-8.084014</td>\n      <td>117.528357</td>\n      <td>-70.880802</td>\n      <td>0</td>\n      <td>0.0760</td>\n      <td>0.0686</td>\n      <td>0.0194</td>\n      <td>37.4496</td>\n      <td>0.028</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7823</th>\n      <td>130330088</td>\n      <td>340.839844</td>\n      <td>-32.442867</td>\n      <td>14.265035</td>\n      <td>-61.816065</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.010</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7824</th>\n      <td>130359176</td>\n      <td>138.164062</td>\n      <td>-40.033035</td>\n      <td>264.026219</td>\n      <td>5.768998</td>\n      <td>0</td>\n      <td>0.0397</td>\n      <td>0.0768</td>\n      <td>0.0149</td>\n      <td>37.7087</td>\n      <td>0.417</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>7825</th>\n      <td>130375489</td>\n      <td>356.132812</td>\n      <td>-27.111860</td>\n      <td>29.160507</td>\n      <td>-75.123425</td>\n      <td>0</td>\n      <td>0.2886</td>\n      <td>0.2939</td>\n      <td>0.0093</td>\n      <td>40.9044</td>\n      <td>0.016</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>7826</th>\n      <td>130386135</td>\n      <td>28.828125</td>\n      <td>-25.117701</td>\n      <td>208.535318</td>\n      <td>-75.532469</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.011</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7827</th>\n      <td>130402542</td>\n      <td>140.976562</td>\n      <td>3.732834</td>\n      <td>228.783975</td>\n      <td>35.301875</td>\n      <td>0</td>\n      <td>0.1518</td>\n      <td>0.1798</td>\n      <td>0.0301</td>\n      <td>39.6994</td>\n      <td>0.032</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7828</th>\n      <td>130408188</td>\n      <td>314.296875</td>\n      <td>-40.033035</td>\n      <td>2.071417</td>\n      <td>-40.445067</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.036</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7829</th>\n      <td>130414189</td>\n      <td>36.480583</td>\n      <td>-51.642994</td>\n      <td>273.384272</td>\n      <td>-59.721065</td>\n      <td>0</td>\n      <td>0.0542</td>\n      <td>0.0816</td>\n      <td>0.0141</td>\n      <td>37.8452</td>\n      <td>0.024</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>7830</th>\n      <td>130489916</td>\n      <td>209.707031</td>\n      <td>-38.300922</td>\n      <td>317.204568</td>\n      <td>22.695931</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.074</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7831</th>\n      <td>130552230</td>\n      <td>54.316406</td>\n      <td>-18.524391</td>\n      <td>209.170030</td>\n      <td>-51.015495</td>\n      <td>0</td>\n      <td>0.1226</td>\n      <td>0.1440</td>\n      <td>0.0172</td>\n      <td>39.1698</td>\n      <td>0.061</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7832</th>\n      <td>130595291</td>\n      <td>304.980469</td>\n      <td>-14.169522</td>\n      <td>29.658085</td>\n      <td>-25.884722</td>\n      <td>0</td>\n      <td>0.2094</td>\n      <td>0.2023</td>\n      <td>0.0118</td>\n      <td>39.9839</td>\n      <td>0.061</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7833</th>\n      <td>130617044</td>\n      <td>68.378906</td>\n      <td>-16.334824</td>\n      <td>213.119133</td>\n      <td>-37.716403</td>\n      <td>0</td>\n      <td>0.1423</td>\n      <td>0.1646</td>\n      <td>0.0113</td>\n      <td>39.4875</td>\n      <td>0.035</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>7834</th>\n      <td>130622528</td>\n      <td>132.539062</td>\n      <td>-1.641510</td>\n      <td>229.063354</td>\n      <td>25.304821</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.015</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7835</th>\n      <td>130639669</td>\n      <td>164.003906</td>\n      <td>-31.039240</td>\n      <td>275.630181</td>\n      <td>25.642742</td>\n      <td>0</td>\n      <td>0.2760</td>\n      <td>0.2847</td>\n      <td>0.0109</td>\n      <td>40.8250</td>\n      <td>0.070</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>7836</th>\n      <td>130659834</td>\n      <td>174.902344</td>\n      <td>-2.089372</td>\n      <td>269.456940</td>\n      <td>56.041993</td>\n      <td>0</td>\n      <td>0.4215</td>\n      <td>0.3775</td>\n      <td>0.0206</td>\n      <td>41.5364</td>\n      <td>0.024</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7837</th>\n      <td>130678775</td>\n      <td>142.734375</td>\n      <td>-23.480536</td>\n      <td>254.430952</td>\n      <td>19.977762</td>\n      <td>0</td>\n      <td>0.0584</td>\n      <td>0.0885</td>\n      <td>0.0149</td>\n      <td>38.0319</td>\n      <td>0.056</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>7838</th>\n      <td>130684460</td>\n      <td>117.949219</td>\n      <td>-0.895283</td>\n      <td>220.752608</td>\n      <td>12.975772</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.053</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7839</th>\n      <td>130695262</td>\n      <td>90.000000</td>\n      <td>-20.264481</td>\n      <td>226.045011</td>\n      <td>-20.105392</td>\n      <td>0</td>\n      <td>0.1273</td>\n      <td>2.6113</td>\n      <td>0.9733</td>\n      <td>46.6624</td>\n      <td>0.043</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7840</th>\n      <td>130698059</td>\n      <td>142.734375</td>\n      <td>-18.997131</td>\n      <td>250.892051</td>\n      <td>23.021357</td>\n      <td>0</td>\n      <td>0.0591</td>\n      <td>0.0569</td>\n      <td>0.0248</td>\n      <td>37.0254</td>\n      <td>0.045</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>7841</th>\n      <td>130716752</td>\n      <td>337.372894</td>\n      <td>-57.209084</td>\n      <td>332.816119</td>\n      <td>-50.876643</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.016</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7842</th>\n      <td>130727624</td>\n      <td>294.960938</td>\n      <td>-4.031936</td>\n      <td>34.856817</td>\n      <td>-12.602145</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.291</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7843</th>\n      <td>130739978</td>\n      <td>26.718750</td>\n      <td>-14.940303</td>\n      <td>172.342697</td>\n      <td>-72.255675</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.013</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7844</th>\n      <td>130755807</td>\n      <td>120.101349</td>\n      <td>-62.696659</td>\n      <td>275.742955</td>\n      <td>-16.509746</td>\n      <td>0</td>\n      <td>0.1725</td>\n      <td>2.5606</td>\n      <td>1.1146</td>\n      <td>46.6108</td>\n      <td>0.136</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>7845</th>\n      <td>130762946</td>\n      <td>203.108109</td>\n      <td>-55.682144</td>\n      <td>308.728904</td>\n      <td>6.727511</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.430</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7846</th>\n      <td>130772921</td>\n      <td>79.101562</td>\n      <td>-35.501846</td>\n      <td>239.172243</td>\n      <td>-33.827844</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.034</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7847</th>\n      <td>130779836</td>\n      <td>301.992188</td>\n      <td>-17.426323</td>\n      <td>25.102988</td>\n      <td>-24.511101</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.091</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>7848 rows Ã— 12 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eab61887d48788fe3e07325394e6e4c3359ee63"
      },
      "cell_type": "code",
      "source": "labels",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "      6   15  16  42  52  53  62  64  65  67  88  90  92  95\n0      0   0   0   0   0   0   0   0   0   0   0   0   1   0\n1      0   0   0   0   0   0   0   0   0   0   1   0   0   0\n2      0   0   0   1   0   0   0   0   0   0   0   0   0   0\n3      0   0   0   0   0   0   0   0   0   0   0   1   0   0\n4      0   0   0   0   0   0   0   0   0   0   0   1   0   0\n5      0   0   0   0   0   0   0   0   1   0   0   0   0   0\n6      0   0   0   0   0   0   0   0   0   0   0   1   0   0\n7      0   0   0   1   0   0   0   0   0   0   0   0   0   0\n8      0   0   0   0   0   0   0   0   0   0   0   1   0   0\n9      0   0   0   0   0   0   0   0   1   0   0   0   0   0\n10     0   0   0   0   0   0   0   0   0   0   0   1   0   0\n11     0   0   0   1   0   0   0   0   0   0   0   0   0   0\n12     0   0   0   1   0   0   0   0   0   0   0   0   0   0\n13     0   0   0   0   0   0   0   0   0   0   0   1   0   0\n14     0   0   0   0   0   0   0   0   1   0   0   0   0   0\n15     0   0   1   0   0   0   0   0   0   0   0   0   0   0\n16     0   0   0   0   0   0   0   0   0   1   0   0   0   0\n17     0   0   0   0   0   0   0   0   0   1   0   0   0   0\n18     0   0   0   1   0   0   0   0   0   0   0   0   0   0\n19     0   0   0   0   0   0   0   0   0   0   0   0   0   1\n20     0   0   0   0   0   0   0   0   0   0   1   0   0   0\n21     0   0   0   0   0   0   1   0   0   0   0   0   0   0\n22     0   0   0   0   0   0   0   0   0   0   1   0   0   0\n23     0   0   0   1   0   0   0   0   0   0   0   0   0   0\n24     0   0   1   0   0   0   0   0   0   0   0   0   0   0\n25     0   1   0   0   0   0   0   0   0   0   0   0   0   0\n26     0   0   0   1   0   0   0   0   0   0   0   0   0   0\n27     0   0   0   0   0   0   0   0   0   0   0   1   0   0\n28     0   0   0   0   0   0   0   0   0   0   0   1   0   0\n29     0   0   0   0   0   0   0   0   0   0   0   1   0   0\n...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n7818   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n7819   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n7820   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n7821   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7822   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n7823   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n7824   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n7825   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n7826   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n7827   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n7828   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7829   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n7830   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7831   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n7832   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n7833   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n7834   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n7835   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n7836   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n7837   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n7838   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7839   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n7840   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n7841   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7842   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n7843   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n7844   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n7845   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n7846   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n7847   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n\n[7848 rows x 14 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>6</th>\n      <th>15</th>\n      <th>16</th>\n      <th>42</th>\n      <th>52</th>\n      <th>53</th>\n      <th>62</th>\n      <th>64</th>\n      <th>65</th>\n      <th>67</th>\n      <th>88</th>\n      <th>90</th>\n      <th>92</th>\n      <th>95</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7818</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7819</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7820</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7821</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7822</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7823</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7824</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7825</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7826</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7827</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7828</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7829</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7830</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7831</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7832</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7833</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7834</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7835</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7836</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7837</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7838</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7839</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7840</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7841</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7842</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7843</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7844</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7845</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7846</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7847</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7848 rows Ã— 14 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0165c5ce25c7dd4db8ca2aab74a55838dc324d9e"
      },
      "cell_type": "code",
      "source": "#TODO split X and y into train/test set. (Maybe a val set ?)\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\nsplits = list(splitter.split(X, y))[0]\ntrain_ind, test_ind = splits",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc30adb31c32d705b3b8da640f3c35c4a3bc4025"
      },
      "cell_type": "code",
      "source": "X_train = X[train_ind]\nX_test  = X[test_ind]\n\ny_train = y[train_ind]\ny_test  = y[test_ind]",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f870ea3a2f7c8e9f0430b367d828a768d486456"
      },
      "cell_type": "code",
      "source": "print(y_train.shape, y_test.shape)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(6278, 14) (1570, 14)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1e66ab09477a76cbfb0abf63945942453038fb0e"
      },
      "cell_type": "markdown",
      "source": "6000 images isn't nearly enough for training a CNN (unless you're doing transfer learning), but we'll have to make do with what we have. Again, feedback appreciated."
    },
    {
      "metadata": {
        "_uuid": "4c6ef68244900ae0840e7f7c57f740398a8eb484"
      },
      "cell_type": "markdown",
      "source": "## The CNN Model Architecture\n\nThe CNN Architecture is outined below. Now, I'm definitely not an expert in building models, but I tried a few different variations and found this to be most well-suited.\n\nA couple of things things I noted -\n- The dimensions of the input image are unusually small even for a CNN (a typical image size is 256x256 per channel whereas we have a 23x24) and my gut feeling was that the first Convolutional layer should not be max pooled. This turned out to be right - I got better training layer in abscence of the max pooling layer.\n- I didn't try a whole bunch of activations, but using an Exponential Linear Unit (ELU) worked better than ordinary Rectilinear Unit (RELU). This is probably because the internal layers have negative values.\n\n**UPDATE:**\nAfter visualizing a few sample images for each class, I noticed that the lack of synchronicity between the passband observations meant that especially for events like supernovae, only some of the bands would capture the event. The remaining bands are essentially noise for the purpose of classification.  In this case a regular convolution kernel involving multiplication across channels might not be able to capture the features. My hunch was if we try to instead convolve individual features separately, we'd stand a better chance to capture those features.\nLuckily, I found Keras had a layer called *Depthwise Convolution* which does exactly what we need. At some point we need to combine those features together, and that's where a *Separable Convolution* comes in, which is essentially Depthwise convolution followed by a regular pointwise convolution. See [Keras Documentation](https://keras.io/layers/convolutional/) for more information about the various Convolutional Layers.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51ccf188ef97a138de85a67e5166ff9e322e74e7"
      },
      "cell_type": "code",
      "source": "def build_model(n_dm_bins=23, n_dt_bins=24, n_passbands=6, n_classes=14):\n    model = Sequential()\n    model.add(DepthwiseConv2D(kernel_size=2, strides=1, depth_multiplier=8,\n                     padding=\"valid\", activation=\"elu\",\n                     input_shape=(n_dm_bins, n_dt_bins, n_passbands)))\n    model.add(Dropout(0.1))\n    model.add(SeparableConv2D(48, kernel_size=2, strides=1, depth_multiplier=2,\n                    padding=\"valid\", activation=\"elu\"))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.1))\n    model.add(Flatten())\n    model.add(Dense(32, activation=\"elu\"))\n    model.add(Dropout(0.25))\n    model.add(Dense(n_classes, activation=\"softmax\"))\n    print(model.summary())\n    return model",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27dc0d25123685a653ac89dc4d18dff4e7e229e8"
      },
      "cell_type": "markdown",
      "source": "## Custom Loss Function\n\nThe code below implements the weighted multi class log loss functions. This is similar to the one defined in the Evaluation section of the project description with one caveat - I have used the proportion of objects in a class instead of the actual counts, since I found it gave more readable loss values.\n\nA couple of Keras-specific things - \n- A custom loss function defined in Keras this way must only methods from the keras.backend interface to process its arguments (and not use numpy directly) to ensure that it can be smoothly translated into the appropriate backend (Tensorflow or theano) when compiling the model. \n- I set Keras to use float32 as the default variable dtype, since float 64 wasn't working for some reason."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b3903329e56325b1d69b51a9029b1b6129c15ca"
      },
      "cell_type": "code",
      "source": "n_classes=14\n#assumes weights to be all ones as actual weights are hidden\n#UPDATE - settings weights for classes 15 (idx=1) and 64(idx=7) to 2 based on LB probing post \n#https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194#397153\nweights = np.ones(n_classes, dtype='float32') \nweights[1], weights[7] = 2, 2\nepsilon = 1e-7\n#number of objects per class\nclass_counts = df_meta.groupby('target')['object_id'].count().values \n#proportion of objects per class\nclass_proportions = class_counts/np.max(class_counts)\n#set backend to float 32\nK.set_floatx('float32')",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80c2fa279b890449a6cb8a01f7dbe913e5d48672"
      },
      "cell_type": "code",
      "source": "#weighted multi-class log loss\ndef weighted_mc_log_loss(y_true, y_pred):\n    y_pred_clipped = K.clip(y_pred, epsilon, 1-epsilon)\n    #true labels weighted by weights and percent elements per class\n    y_true_weighted = (y_true * weights)/class_proportions\n    #multiply tensors element-wise and then sum\n    loss_num = (y_true_weighted * K.log(y_pred_clipped))\n    loss = -1*K.sum(loss_num)/K.sum(weights)\n    \n    return loss",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e4a3e290c48f058097f66c332bd0d0535dceb1f1"
      },
      "cell_type": "markdown",
      "source": "This is just small test of the loss function to see if its compiles and runs fine."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63f0edc11a09dcbd2a4fa270470e18c34c3370a5"
      },
      "cell_type": "code",
      "source": "y_true = K.variable(np.eye(14, dtype='float32'))\ny_pred = K.variable(np.eye(14, dtype='float32'))\nres = weighted_mc_log_loss(y_true, y_pred)\nK.eval(res)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "1.5840004e-06"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ca77c4b740eb8c605f24a13044aa4e9d8f5dcbfc"
      },
      "cell_type": "markdown",
      "source": "## Training and Evaluation\n\nLets build the model now."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a55b64691516beb10d09ba06a6609013d88c483"
      },
      "cell_type": "code",
      "source": "model = build_model()",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndepthwise_conv2d_1 (Depthwis (None, 22, 23, 48)        240       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 22, 23, 48)        0         \n_________________________________________________________________\nseparable_conv2d_1 (Separabl (None, 21, 22, 48)        5040      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 10, 11, 48)        0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 10, 11, 48)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 5280)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                168992    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 14)                462       \n=================================================================\nTotal params: 174,734\nTrainable params: 174,734\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0181f91a3fcebea3139c2948f117d6901e2f41c2"
      },
      "cell_type": "markdown",
      "source": "I also print the model summary to get an idea of the number of parameters to be trained. 50000 is a lot of parameters for just 6k input rows, but its nothing compared to the CNNs used for image classification. I found higher number of parameters gave worse results on test.\n\nLets compile the model now, I used Adam as the optimizer based on the paper, but I increased the learning rate to 0.002 since it yielded faster convergence."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dcf3b58a02b8ccf8edaf4a84fea202fd8b2368f"
      },
      "cell_type": "code",
      "source": "model.compile(loss=weighted_mc_log_loss, optimizer=Adam(lr=0.002), metrics=['accuracy'])",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "365a9f4d9033a96514161f9930aa341e3b7726e9"
      },
      "cell_type": "markdown",
      "source": "Training for 20 epochs (I didn't do a validation set since the training set is already too small). I found that higher epochs led to overfitting."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62bdae5e8f6370fa299ec6f6fb72017e8dd7b8e1"
      },
      "cell_type": "code",
      "source": "checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=1)",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "841f7c95d9c8fc7df7afd58a9a9f12712a85b9d4"
      },
      "cell_type": "code",
      "source": "class ReduceLRWithEarlyStopping(ReduceLROnPlateau):\n    def __init__(self, *args, **kwargs):\n        super(ReduceLRWithEarlyStopping, self).__init__(*args, **kwargs)\n        self.stopped_epoch = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        super(ReduceLRWithEarlyStopping, self).on_epoch_end(epoch, logs)\n        old_lr = float(K.get_value(self.model.optimizer.lr))\n        if self.wait >= self.patience and old_lr <= self.min_lr:\n            # Stop training early\n            self.stopped_epoch = epoch\n            self.model.stop_training = True\n\n    def on_train_end(self, logs=None):\n        super(ReduceLRWithEarlyStopping, self).on_epoch_end(logs)\n        if self.stopped_epoch > 0 and self.verbose > 0:\n            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "389ea729853a0e31556bed93527bcf8867cd039e"
      },
      "cell_type": "code",
      "source": "reduce_lr = ReduceLRWithEarlyStopping(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.00001)",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c6e22a858acba60135b6ef64a34bd74c6b191fd"
      },
      "cell_type": "code",
      "source": "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=[X_test, y_test],shuffle=True,verbose=1,callbacks=[checkPoint, reduce_lr])",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 6278 samples, validate on 1570 samples\nEpoch 1/100\n6278/6278 [==============================] - 5s 858us/step - loss: 22.1453 - acc: 0.1743 - val_loss: 16.3393 - val_acc: 0.2904\n\nEpoch 00001: val_loss improved from inf to 16.33933, saving model to ./keras.model\nEpoch 2/100\n6278/6278 [==============================] - 2s 387us/step - loss: 16.4775 - acc: 0.2732 - val_loss: 14.3446 - val_acc: 0.3777\n\nEpoch 00002: val_loss improved from 16.33933 to 14.34459, saving model to ./keras.model\nEpoch 3/100\n6278/6278 [==============================] - 2s 386us/step - loss: 13.7198 - acc: 0.3224 - val_loss: 13.1333 - val_acc: 0.3739\n\nEpoch 00003: val_loss improved from 14.34459 to 13.13334, saving model to ./keras.model\nEpoch 4/100\n6278/6278 [==============================] - 2s 384us/step - loss: 11.3791 - acc: 0.3807 - val_loss: 14.0429 - val_acc: 0.3503\n\nEpoch 00004: val_loss did not improve from 13.13334\nEpoch 5/100\n6278/6278 [==============================] - 2s 385us/step - loss: 10.1562 - acc: 0.4165 - val_loss: 13.1883 - val_acc: 0.3433\n\nEpoch 00005: val_loss did not improve from 13.13334\nEpoch 6/100\n6278/6278 [==============================] - 2s 389us/step - loss: 8.9457 - acc: 0.4667 - val_loss: 13.6635 - val_acc: 0.4363\n\nEpoch 00006: val_loss did not improve from 13.13334\nEpoch 7/100\n6278/6278 [==============================] - 2s 385us/step - loss: 8.2882 - acc: 0.4951 - val_loss: 13.7669 - val_acc: 0.4293\n\nEpoch 00007: val_loss did not improve from 13.13334\nEpoch 8/100\n6278/6278 [==============================] - 2s 383us/step - loss: 7.6909 - acc: 0.5018 - val_loss: 15.6291 - val_acc: 0.4102\n\nEpoch 00008: val_loss did not improve from 13.13334\nEpoch 9/100\n6278/6278 [==============================] - 2s 384us/step - loss: 6.0314 - acc: 0.5739 - val_loss: 15.0230 - val_acc: 0.4847\n\nEpoch 00009: val_loss did not improve from 13.13334\nEpoch 10/100\n6278/6278 [==============================] - 2s 384us/step - loss: 5.2672 - acc: 0.6010 - val_loss: 15.3864 - val_acc: 0.5045\n\nEpoch 00010: val_loss did not improve from 13.13334\nEpoch 11/100\n6278/6278 [==============================] - 2s 384us/step - loss: 4.9133 - acc: 0.6228 - val_loss: 15.5434 - val_acc: 0.5096\n\nEpoch 00011: val_loss did not improve from 13.13334\nEpoch 12/100\n6278/6278 [==============================] - 2s 385us/step - loss: 4.7502 - acc: 0.6305 - val_loss: 16.5161 - val_acc: 0.5236\n\nEpoch 00012: val_loss did not improve from 13.13334\nEpoch 13/100\n6278/6278 [==============================] - 2s 385us/step - loss: 4.5496 - acc: 0.6469 - val_loss: 16.4800 - val_acc: 0.5185\n\nEpoch 00013: val_loss did not improve from 13.13334\nEpoch 14/100\n6278/6278 [==============================] - 2s 385us/step - loss: 4.3168 - acc: 0.6523 - val_loss: 16.4449 - val_acc: 0.5287\n\nEpoch 00014: val_loss did not improve from 13.13334\nEpoch 15/100\n6278/6278 [==============================] - 2s 386us/step - loss: 4.0836 - acc: 0.6594 - val_loss: 17.0076 - val_acc: 0.5293\n\nEpoch 00015: val_loss did not improve from 13.13334\nEpoch 16/100\n6278/6278 [==============================] - 2s 385us/step - loss: 4.0422 - acc: 0.6657 - val_loss: 16.9720 - val_acc: 0.5433\n\nEpoch 00016: val_loss did not improve from 13.13334\nEpoch 17/100\n6278/6278 [==============================] - 2s 383us/step - loss: 4.0317 - acc: 0.6733 - val_loss: 16.9253 - val_acc: 0.5338\n\nEpoch 00017: val_loss did not improve from 13.13334\nEpoch 18/100\n6278/6278 [==============================] - 2s 384us/step - loss: 3.9833 - acc: 0.6711 - val_loss: 17.1532 - val_acc: 0.5350\n\nEpoch 00018: val_loss did not improve from 13.13334\nEpoch 19/100\n6278/6278 [==============================] - 2s 385us/step - loss: 4.0513 - acc: 0.6704 - val_loss: 17.0897 - val_acc: 0.5389\n\nEpoch 00019: val_loss did not improve from 13.13334\nEpoch 20/100\n6278/6278 [==============================] - 2s 383us/step - loss: 4.0139 - acc: 0.6709 - val_loss: 16.9837 - val_acc: 0.5369\n\nEpoch 00020: val_loss did not improve from 13.13334\nEpoch 21/100\n6278/6278 [==============================] - 2s 386us/step - loss: 3.8962 - acc: 0.6808 - val_loss: 17.0401 - val_acc: 0.5408\n\nEpoch 00021: val_loss did not improve from 13.13334\nEpoch 22/100\n6278/6278 [==============================] - 2s 383us/step - loss: 3.9372 - acc: 0.6817 - val_loss: 17.0696 - val_acc: 0.5401\n\nEpoch 00022: val_loss did not improve from 13.13334\nEpoch 23/100\n6278/6278 [==============================] - 2s 385us/step - loss: 3.8297 - acc: 0.6701 - val_loss: 17.1883 - val_acc: 0.5427\n\nEpoch 00023: val_loss did not improve from 13.13334\nEpoch 24/100\n6278/6278 [==============================] - 2s 384us/step - loss: 3.9918 - acc: 0.6749 - val_loss: 17.1468 - val_acc: 0.5395\n\nEpoch 00024: val_loss did not improve from 13.13334\nEpoch 25/100\n6278/6278 [==============================] - 2s 383us/step - loss: 3.9250 - acc: 0.6813 - val_loss: 17.0812 - val_acc: 0.5408\n\nEpoch 00025: val_loss did not improve from 13.13334\nEpoch 26/100\n6278/6278 [==============================] - 2s 384us/step - loss: 3.9378 - acc: 0.6803 - val_loss: 17.0516 - val_acc: 0.5439\n\nEpoch 00026: val_loss did not improve from 13.13334\nEpoch 27/100\n6278/6278 [==============================] - 2s 385us/step - loss: 3.9474 - acc: 0.6798 - val_loss: 17.0879 - val_acc: 0.5427\n\nEpoch 00027: val_loss did not improve from 13.13334\nEpoch 28/100\n6278/6278 [==============================] - 2s 385us/step - loss: 3.8423 - acc: 0.6735 - val_loss: 17.1733 - val_acc: 0.5433\n\nEpoch 00028: val_loss did not improve from 13.13334\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/callbacks.py:1106: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: lr\n  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90b175133bec0d7ebf8fb0812f7029f18e7d310c"
      },
      "cell_type": "code",
      "source": "def plot_loss_acc(history):\n    plt.plot(history['loss'][1:])\n    plt.plot(history['val_loss'][1:])\n    plt.title('model loss')\n    plt.ylabel('val_loss')\n    plt.xlabel('epoch')\n    plt.legend(['train','Validation'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history['acc'][1:])\n    plt.plot(history['val_acc'][1:])\n    plt.title('model Accuracy')\n    plt.ylabel('val_acc')\n    plt.xlabel('epoch')\n    plt.legend(['train','Validation'], loc='upper left')\n    plt.show()",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca4ff4335e0775f550d5312b4d3a97eae3e7b7ac"
      },
      "cell_type": "code",
      "source": "plot_loss_acc(history.history)",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2e32e528b8f5c94fceaeb38d618fb4b88248c1bc"
      },
      "cell_type": "markdown",
      "source": "Lets evaluate it on the test set now."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1f5f6d4f41efb8475a58d9882a91053422c095a"
      },
      "cell_type": "code",
      "source": "loss_acc = model.evaluate(X_test, y_test, batch_size=32)\nprint(loss_acc)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1570/1570 [==============================] - 0s 127us/step\n[17.17332653923399, 0.543312101910828]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0175af53770517c2ceb675b580536e509376af9d"
      },
      "cell_type": "markdown",
      "source": "~67% accuracy on the training set, and ~55% on the test is good but not great, so really any feedback here is truly appreciated.\n\nLets try and look at some of the predictions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "077aff3808a1d8a10f329e0f98d522b2d5d4cd78"
      },
      "cell_type": "code",
      "source": "y_pred_test = model.predict(X_test)",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5d124b97c372f81dabd19c922ee7abbc0f064e6"
      },
      "cell_type": "code",
      "source": "classes = np.sort(df_meta['target'].drop_duplicates())",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe174f31e195e814edea67a02ea2524f885e3a1a"
      },
      "cell_type": "code",
      "source": "df_meta_test = df_meta.iloc[test_ind]\ndf_meta_test['pred_label'] = classes[np.argmax(y_pred_test, axis=1)]",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48d1132f0038c6636a5c7e643fd4fa3e8786b250"
      },
      "cell_type": "code",
      "source": "df_meta_test.loc[df_meta_test.target == 15]",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "      object_id          ra       decl     ...      mwebv  target  pred_label\n5059   67875060  250.312500 -23.155577     ...      0.456      15          15\n6246   94529982  351.309021 -46.375080     ...      0.006      15          15\n6438   99179619   55.898438 -10.958863     ...      0.049      15          15\n6818  107732897    0.351562 -38.491302     ...      0.012      15           6\n3795   38505257  328.886719 -11.110882     ...      0.026      15          65\n3365   28492953   72.773438 -22.185358     ...      0.038      15          15\n7381  120272506   19.335938 -13.094776     ...      0.018      15          15\n6157   92529037  132.011719  -9.896853     ...      0.048      15          42\n7640  126270180  199.687500 -40.033035     ...      0.089      15           6\n3010   20432463  359.472656  -2.388015     ...      0.033      15          15\n3896   40757084  359.472656  -1.790785     ...      0.038      15          15\n7724  128313218  127.792969 -19.155010     ...      0.094      15          15\n2763   15322511  335.566406 -15.713861     ...      0.029      15          65\n6810  107609019   30.211267 -50.285809     ...      0.017      15          62\n6739  105832546  187.910156 -32.442867     ...      0.082      15          90\n4876   63541939   67.324219  -3.284369     ...      0.047      15          15\n6429   98929706  176.835938 -12.177298     ...      0.026      15          15\n3432   29933389  343.476562  -1.044512     ...      0.070      15          15\n3042   21007001  185.625000 -22.185358     ...      0.061      15          15\n2603   11471117   82.905403 -48.532253     ...      0.025      15          65\n4377   51872509  110.390625   1.940072     ...      0.146      15          15\n2430    7571956  317.109375 -17.113819     ...      0.078      15          15\n5826   85397800  176.132812 -25.117701     ...      0.049      15          15\n4154   46880215  180.351562  -7.331168     ...      0.021      15          67\n7704  127884877   22.148438 -20.902014     ...      0.016      15          15\n2948   19179463  100.371094 -37.544327     ...      0.095      15          42\n7549  124218782   58.007812 -38.873585     ...      0.007      15          62\n7827  130402542  140.976562   3.732834     ...      0.032      15          42\n6973  111079035  260.537384 -50.091457     ...      0.202      15          65\n3334   27698813   12.469880 -43.207821     ...      0.009      15          15\n...         ...         ...        ...     ...        ...     ...         ...\n4397   52429042   22.324219  -2.388015     ...      0.036      15          15\n6953  110574404  124.046051 -61.943836     ...      0.158      15           6\n6502  100826219    1.757812 -27.784405     ...      0.014      15          15\n3534   32503295   59.414062 -15.249383     ...      0.042      15          62\n2760   15277248  272.704926 -56.064457     ...      0.093      15          15\n7195  116304152  101.565422 -50.091457     ...      0.058      15          42\n6042   90048656   51.679688 -38.491302     ...      0.016      15           6\n3741   37268602  100.195312 -17.113819     ...      0.346      15          16\n5759   83890499   93.947365 -58.350353     ...      0.035      15          62\n5043   67466710    7.207031 -37.168900     ...      0.013      15          15\n3297   26680984   46.346153 -46.178181     ...      0.009      15          65\n7784  129335528    2.636719 -11.415158     ...      0.027      15          15\n2491    8847522   68.906250 -18.997131     ...      0.029      15          15\n5353   74888577  295.312500 -34.048595     ...      0.123      15          15\n5784   84461964   87.363281 -16.334824     ...      0.076      15          15\n2458    8257748  124.980469   0.000000     ...      0.033      15          67\n6680  104610548  194.765625 -28.461132     ...      0.064      15          15\n4790   61677556  152.578125 -38.491302     ...      0.085      15          15\n5230   71928039  212.695312 -10.351558     ...      0.045      15          15\n5030   67211738  208.652344 -19.471222     ...      0.086      15          15\n7241  117264461  173.144531 -39.065495     ...      0.105      15          65\n4599   57128874   14.765625  -6.129201     ...      0.054      15          15\n6683  104711764   60.468750 -25.117701     ...      0.028      15          15\n7730  128439785   59.414062 -38.491302     ...      0.004      15          42\n5954   87945160  235.195312 -20.264481     ...      0.143      15          15\n6210   93834540   80.419357 -61.378410     ...      0.030      15          15\n3120   22540174  359.472656 -25.944481     ...      0.017      15          15\n6093   91022592  162.773438 -40.033035     ...      0.097      15          65\n6080   90750519  130.781250  -1.044512     ...      0.027      15          15\n3728   37077473  132.714844  -0.298417     ...      0.026      15          62\n\n[99 rows x 13 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_id</th>\n      <th>ra</th>\n      <th>decl</th>\n      <th>gal_l</th>\n      <th>gal_b</th>\n      <th>ddf</th>\n      <th>hostgal_specz</th>\n      <th>hostgal_photoz</th>\n      <th>hostgal_photoz_err</th>\n      <th>distmod</th>\n      <th>mwebv</th>\n      <th>target</th>\n      <th>pred_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5059</th>\n      <td>67875060</td>\n      <td>250.312500</td>\n      <td>-23.155577</td>\n      <td>356.335534</td>\n      <td>15.125077</td>\n      <td>0</td>\n      <td>0.1537</td>\n      <td>0.1746</td>\n      <td>0.0176</td>\n      <td>39.6289</td>\n      <td>0.456</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6246</th>\n      <td>94529982</td>\n      <td>351.309021</td>\n      <td>-46.375080</td>\n      <td>338.712954</td>\n      <td>-64.313996</td>\n      <td>0</td>\n      <td>0.1890</td>\n      <td>0.2642</td>\n      <td>0.1869</td>\n      <td>40.6391</td>\n      <td>0.006</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6438</th>\n      <td>99179619</td>\n      <td>55.898438</td>\n      <td>-10.958863</td>\n      <td>199.663279</td>\n      <td>-46.494255</td>\n      <td>0</td>\n      <td>0.1689</td>\n      <td>0.1972</td>\n      <td>0.0146</td>\n      <td>39.9226</td>\n      <td>0.049</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6818</th>\n      <td>107732897</td>\n      <td>0.351562</td>\n      <td>-38.491302</td>\n      <td>342.451146</td>\n      <td>-74.550474</td>\n      <td>0</td>\n      <td>0.0561</td>\n      <td>0.0582</td>\n      <td>0.0184</td>\n      <td>37.0774</td>\n      <td>0.012</td>\n      <td>15</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3795</th>\n      <td>38505257</td>\n      <td>328.886719</td>\n      <td>-11.110882</td>\n      <td>45.388452</td>\n      <td>-45.755108</td>\n      <td>0</td>\n      <td>0.9473</td>\n      <td>0.9194</td>\n      <td>0.0154</td>\n      <td>43.8748</td>\n      <td>0.026</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3365</th>\n      <td>28492953</td>\n      <td>72.773438</td>\n      <td>-22.185358</td>\n      <td>221.757534</td>\n      <td>-35.824363</td>\n      <td>0</td>\n      <td>0.0340</td>\n      <td>0.0251</td>\n      <td>0.0174</td>\n      <td>35.1955</td>\n      <td>0.038</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7381</th>\n      <td>120272506</td>\n      <td>19.335938</td>\n      <td>-13.094776</td>\n      <td>147.559502</td>\n      <td>-74.714083</td>\n      <td>0</td>\n      <td>0.4065</td>\n      <td>0.4568</td>\n      <td>0.0430</td>\n      <td>42.0262</td>\n      <td>0.018</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6157</th>\n      <td>92529037</td>\n      <td>132.011719</td>\n      <td>-9.896853</td>\n      <td>236.303113</td>\n      <td>20.408904</td>\n      <td>0</td>\n      <td>0.5069</td>\n      <td>0.3451</td>\n      <td>0.0577</td>\n      <td>41.3080</td>\n      <td>0.048</td>\n      <td>15</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7640</th>\n      <td>126270180</td>\n      <td>199.687500</td>\n      <td>-40.033035</td>\n      <td>308.588085</td>\n      <td>22.538567</td>\n      <td>0</td>\n      <td>0.1886</td>\n      <td>2.8631</td>\n      <td>1.2494</td>\n      <td>46.9040</td>\n      <td>0.089</td>\n      <td>15</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3010</th>\n      <td>20432463</td>\n      <td>359.472656</td>\n      <td>-2.388015</td>\n      <td>93.266772</td>\n      <td>-62.135302</td>\n      <td>0</td>\n      <td>0.1252</td>\n      <td>0.1247</td>\n      <td>0.0114</td>\n      <td>38.8296</td>\n      <td>0.033</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3896</th>\n      <td>40757084</td>\n      <td>359.472656</td>\n      <td>-1.790785</td>\n      <td>93.820321</td>\n      <td>-61.598121</td>\n      <td>0</td>\n      <td>0.3243</td>\n      <td>0.3207</td>\n      <td>0.0100</td>\n      <td>41.1229</td>\n      <td>0.038</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7724</th>\n      <td>128313218</td>\n      <td>127.792969</td>\n      <td>-19.155010</td>\n      <td>241.870229</td>\n      <td>11.817580</td>\n      <td>0</td>\n      <td>0.1864</td>\n      <td>0.5961</td>\n      <td>0.1179</td>\n      <td>42.7220</td>\n      <td>0.094</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2763</th>\n      <td>15322511</td>\n      <td>335.566406</td>\n      <td>-15.713861</td>\n      <td>43.437800</td>\n      <td>-53.616157</td>\n      <td>0</td>\n      <td>0.3029</td>\n      <td>2.9339</td>\n      <td>1.3001</td>\n      <td>46.9679</td>\n      <td>0.029</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>6810</th>\n      <td>107609019</td>\n      <td>30.211267</td>\n      <td>-50.285809</td>\n      <td>277.816483</td>\n      <td>-63.322513</td>\n      <td>0</td>\n      <td>0.0741</td>\n      <td>1.1027</td>\n      <td>0.7373</td>\n      <td>44.3628</td>\n      <td>0.017</td>\n      <td>15</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>6739</th>\n      <td>105832546</td>\n      <td>187.910156</td>\n      <td>-32.442867</td>\n      <td>298.097291</td>\n      <td>30.242969</td>\n      <td>0</td>\n      <td>0.4017</td>\n      <td>0.3770</td>\n      <td>0.0282</td>\n      <td>41.5332</td>\n      <td>0.082</td>\n      <td>15</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>4876</th>\n      <td>63541939</td>\n      <td>67.324219</td>\n      <td>-3.284369</td>\n      <td>198.201539</td>\n      <td>-32.856643</td>\n      <td>0</td>\n      <td>0.8281</td>\n      <td>0.8249</td>\n      <td>0.0264</td>\n      <td>43.5843</td>\n      <td>0.047</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6429</th>\n      <td>98929706</td>\n      <td>176.835938</td>\n      <td>-12.177298</td>\n      <td>279.282201</td>\n      <td>47.729658</td>\n      <td>0</td>\n      <td>0.1817</td>\n      <td>0.1852</td>\n      <td>0.0072</td>\n      <td>39.7710</td>\n      <td>0.026</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3432</th>\n      <td>29933389</td>\n      <td>343.476562</td>\n      <td>-1.044512</td>\n      <td>70.769616</td>\n      <td>-51.599079</td>\n      <td>0</td>\n      <td>0.5024</td>\n      <td>0.5396</td>\n      <td>0.2011</td>\n      <td>42.4600</td>\n      <td>0.070</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3042</th>\n      <td>21007001</td>\n      <td>185.625000</td>\n      <td>-22.185358</td>\n      <td>294.151329</td>\n      <td>40.192501</td>\n      <td>0</td>\n      <td>0.5839</td>\n      <td>0.5946</td>\n      <td>0.1065</td>\n      <td>42.7154</td>\n      <td>0.061</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2603</th>\n      <td>11471117</td>\n      <td>82.905403</td>\n      <td>-48.532253</td>\n      <td>255.104101</td>\n      <td>-32.874918</td>\n      <td>0</td>\n      <td>0.1070</td>\n      <td>0.1090</td>\n      <td>0.0097</td>\n      <td>38.5148</td>\n      <td>0.025</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>4377</th>\n      <td>51872509</td>\n      <td>110.390625</td>\n      <td>1.940072</td>\n      <td>214.643082</td>\n      <td>7.586134</td>\n      <td>0</td>\n      <td>0.4202</td>\n      <td>0.4312</td>\n      <td>0.0075</td>\n      <td>41.8773</td>\n      <td>0.146</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2430</th>\n      <td>7571956</td>\n      <td>317.109375</td>\n      <td>-17.113819</td>\n      <td>31.725884</td>\n      <td>-37.798966</td>\n      <td>0</td>\n      <td>0.3733</td>\n      <td>0.4342</td>\n      <td>0.5979</td>\n      <td>41.8951</td>\n      <td>0.078</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5826</th>\n      <td>85397800</td>\n      <td>176.132812</td>\n      <td>-25.117701</td>\n      <td>284.306123</td>\n      <td>35.322454</td>\n      <td>0</td>\n      <td>0.1801</td>\n      <td>2.1042</td>\n      <td>1.1141</td>\n      <td>46.0920</td>\n      <td>0.049</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4154</th>\n      <td>46880215</td>\n      <td>180.351562</td>\n      <td>-7.331168</td>\n      <td>281.776857</td>\n      <td>53.473044</td>\n      <td>0</td>\n      <td>0.2964</td>\n      <td>0.2984</td>\n      <td>0.0093</td>\n      <td>40.9421</td>\n      <td>0.021</td>\n      <td>15</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>7704</th>\n      <td>127884877</td>\n      <td>22.148438</td>\n      <td>-20.902014</td>\n      <td>178.630413</td>\n      <td>-79.482393</td>\n      <td>0</td>\n      <td>0.2631</td>\n      <td>0.5315</td>\n      <td>0.2427</td>\n      <td>42.4206</td>\n      <td>0.016</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2948</th>\n      <td>19179463</td>\n      <td>100.371094</td>\n      <td>-37.544327</td>\n      <td>246.549315</td>\n      <td>-17.969007</td>\n      <td>0</td>\n      <td>0.1731</td>\n      <td>0.2256</td>\n      <td>0.2355</td>\n      <td>40.2503</td>\n      <td>0.095</td>\n      <td>15</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>7549</th>\n      <td>124218782</td>\n      <td>58.007812</td>\n      <td>-38.873585</td>\n      <td>242.110293</td>\n      <td>-50.791504</td>\n      <td>0</td>\n      <td>0.0340</td>\n      <td>0.0668</td>\n      <td>0.0261</td>\n      <td>37.3910</td>\n      <td>0.007</td>\n      <td>15</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>7827</th>\n      <td>130402542</td>\n      <td>140.976562</td>\n      <td>3.732834</td>\n      <td>228.783975</td>\n      <td>35.301875</td>\n      <td>0</td>\n      <td>0.1518</td>\n      <td>0.1798</td>\n      <td>0.0301</td>\n      <td>39.6994</td>\n      <td>0.032</td>\n      <td>15</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>6973</th>\n      <td>111079035</td>\n      <td>260.537384</td>\n      <td>-50.091457</td>\n      <td>339.715881</td>\n      <td>-7.637440</td>\n      <td>0</td>\n      <td>0.2431</td>\n      <td>0.2486</td>\n      <td>0.0350</td>\n      <td>40.4880</td>\n      <td>0.202</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3334</th>\n      <td>27698813</td>\n      <td>12.469880</td>\n      <td>-43.207821</td>\n      <td>303.957039</td>\n      <td>-73.917329</td>\n      <td>0</td>\n      <td>0.3475</td>\n      <td>0.4541</td>\n      <td>0.3268</td>\n      <td>42.0112</td>\n      <td>0.009</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4397</th>\n      <td>52429042</td>\n      <td>22.324219</td>\n      <td>-2.388015</td>\n      <td>144.658614</td>\n      <td>-63.651311</td>\n      <td>0</td>\n      <td>0.2641</td>\n      <td>0.2844</td>\n      <td>0.0196</td>\n      <td>40.8216</td>\n      <td>0.036</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6953</th>\n      <td>110574404</td>\n      <td>124.046051</td>\n      <td>-61.943836</td>\n      <td>275.991503</td>\n      <td>-14.543638</td>\n      <td>0</td>\n      <td>0.1011</td>\n      <td>0.0879</td>\n      <td>0.0214</td>\n      <td>38.0162</td>\n      <td>0.158</td>\n      <td>15</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6502</th>\n      <td>100826219</td>\n      <td>1.757812</td>\n      <td>-27.784405</td>\n      <td>26.567507</td>\n      <td>-80.130345</td>\n      <td>0</td>\n      <td>0.1909</td>\n      <td>0.2228</td>\n      <td>0.0138</td>\n      <td>40.2192</td>\n      <td>0.014</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3534</th>\n      <td>32503295</td>\n      <td>59.414062</td>\n      <td>-15.249383</td>\n      <td>207.351690</td>\n      <td>-45.267181</td>\n      <td>0</td>\n      <td>0.0657</td>\n      <td>0.0324</td>\n      <td>0.0338</td>\n      <td>35.7652</td>\n      <td>0.042</td>\n      <td>15</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>2760</th>\n      <td>15277248</td>\n      <td>272.704926</td>\n      <td>-56.064457</td>\n      <td>337.984014</td>\n      <td>-16.900984</td>\n      <td>0</td>\n      <td>0.2721</td>\n      <td>0.2582</td>\n      <td>0.1860</td>\n      <td>40.5820</td>\n      <td>0.093</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7195</th>\n      <td>116304152</td>\n      <td>101.565422</td>\n      <td>-50.091457</td>\n      <td>259.439588</td>\n      <td>-21.264025</td>\n      <td>0</td>\n      <td>0.7281</td>\n      <td>0.6615</td>\n      <td>0.0441</td>\n      <td>42.9968</td>\n      <td>0.058</td>\n      <td>15</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>6042</th>\n      <td>90048656</td>\n      <td>51.679688</td>\n      <td>-38.491302</td>\n      <td>242.277455</td>\n      <td>-55.744136</td>\n      <td>0</td>\n      <td>0.1507</td>\n      <td>0.0983</td>\n      <td>0.0194</td>\n      <td>38.2747</td>\n      <td>0.016</td>\n      <td>15</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3741</th>\n      <td>37268602</td>\n      <td>100.195312</td>\n      <td>-17.113819</td>\n      <td>227.135739</td>\n      <td>-10.004133</td>\n      <td>0</td>\n      <td>0.0687</td>\n      <td>0.0677</td>\n      <td>0.0036</td>\n      <td>37.4196</td>\n      <td>0.346</td>\n      <td>15</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>5759</th>\n      <td>83890499</td>\n      <td>93.947365</td>\n      <td>-58.350353</td>\n      <td>267.199579</td>\n      <td>-27.420246</td>\n      <td>0</td>\n      <td>0.1286</td>\n      <td>0.1029</td>\n      <td>0.0175</td>\n      <td>38.3810</td>\n      <td>0.035</td>\n      <td>15</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>5043</th>\n      <td>67466710</td>\n      <td>7.207031</td>\n      <td>-37.168900</td>\n      <td>326.952236</td>\n      <td>-78.883140</td>\n      <td>0</td>\n      <td>0.3923</td>\n      <td>0.4837</td>\n      <td>0.9100</td>\n      <td>42.1747</td>\n      <td>0.013</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3297</th>\n      <td>26680984</td>\n      <td>46.346153</td>\n      <td>-46.178181</td>\n      <td>257.688093</td>\n      <td>-57.452613</td>\n      <td>0</td>\n      <td>0.2240</td>\n      <td>0.2124</td>\n      <td>0.0143</td>\n      <td>40.1021</td>\n      <td>0.009</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7784</th>\n      <td>129335528</td>\n      <td>2.636719</td>\n      <td>-11.415158</td>\n      <td>89.521266</td>\n      <td>-71.582755</td>\n      <td>0</td>\n      <td>0.0897</td>\n      <td>2.4455</td>\n      <td>0.7594</td>\n      <td>46.4897</td>\n      <td>0.027</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2491</th>\n      <td>8847522</td>\n      <td>68.906250</td>\n      <td>-18.997131</td>\n      <td>216.500005</td>\n      <td>-38.201694</td>\n      <td>0</td>\n      <td>0.1568</td>\n      <td>0.1407</td>\n      <td>0.6962</td>\n      <td>39.1142</td>\n      <td>0.029</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5353</th>\n      <td>74888577</td>\n      <td>295.312500</td>\n      <td>-34.048595</td>\n      <td>5.674562</td>\n      <td>-24.476381</td>\n      <td>0</td>\n      <td>0.1021</td>\n      <td>0.0820</td>\n      <td>0.0212</td>\n      <td>37.8589</td>\n      <td>0.123</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5784</th>\n      <td>84461964</td>\n      <td>87.363281</td>\n      <td>-16.334824</td>\n      <td>221.140486</td>\n      <td>-20.881069</td>\n      <td>0</td>\n      <td>0.1180</td>\n      <td>2.6993</td>\n      <td>1.0961</td>\n      <td>46.7495</td>\n      <td>0.076</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2458</th>\n      <td>8257748</td>\n      <td>124.980469</td>\n      <td>0.000000</td>\n      <td>223.432557</td>\n      <td>19.580880</td>\n      <td>0</td>\n      <td>0.0850</td>\n      <td>0.0846</td>\n      <td>0.0027</td>\n      <td>37.9282</td>\n      <td>0.033</td>\n      <td>15</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>6680</th>\n      <td>104610548</td>\n      <td>194.765625</td>\n      <td>-28.461132</td>\n      <td>304.962463</td>\n      <td>34.380553</td>\n      <td>0</td>\n      <td>0.2216</td>\n      <td>0.2040</td>\n      <td>0.0089</td>\n      <td>40.0042</td>\n      <td>0.064</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4790</th>\n      <td>61677556</td>\n      <td>152.578125</td>\n      <td>-38.491302</td>\n      <td>271.444221</td>\n      <td>14.336479</td>\n      <td>0</td>\n      <td>0.0721</td>\n      <td>0.0780</td>\n      <td>0.0177</td>\n      <td>37.7423</td>\n      <td>0.085</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5230</th>\n      <td>71928039</td>\n      <td>212.695312</td>\n      <td>-10.351558</td>\n      <td>332.773754</td>\n      <td>47.869866</td>\n      <td>0</td>\n      <td>0.4907</td>\n      <td>0.3892</td>\n      <td>0.9458</td>\n      <td>41.6142</td>\n      <td>0.045</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5030</th>\n      <td>67211738</td>\n      <td>208.652344</td>\n      <td>-19.471222</td>\n      <td>322.793102</td>\n      <td>40.951516</td>\n      <td>0</td>\n      <td>0.2983</td>\n      <td>0.5611</td>\n      <td>0.1945</td>\n      <td>42.5627</td>\n      <td>0.086</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7241</th>\n      <td>117264461</td>\n      <td>173.144531</td>\n      <td>-39.065495</td>\n      <td>286.605441</td>\n      <td>21.293155</td>\n      <td>0</td>\n      <td>0.1421</td>\n      <td>0.1630</td>\n      <td>0.0218</td>\n      <td>39.4645</td>\n      <td>0.105</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>4599</th>\n      <td>57128874</td>\n      <td>14.765625</td>\n      <td>-6.129201</td>\n      <td>128.208476</td>\n      <td>-68.922798</td>\n      <td>0</td>\n      <td>0.2897</td>\n      <td>0.3717</td>\n      <td>0.1618</td>\n      <td>41.4966</td>\n      <td>0.054</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6683</th>\n      <td>104711764</td>\n      <td>60.468750</td>\n      <td>-25.117701</td>\n      <td>221.389467</td>\n      <td>-47.462209</td>\n      <td>0</td>\n      <td>0.3078</td>\n      <td>0.3162</td>\n      <td>0.0326</td>\n      <td>41.0876</td>\n      <td>0.028</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>7730</th>\n      <td>128439785</td>\n      <td>59.414062</td>\n      <td>-38.491302</td>\n      <td>241.428426</td>\n      <td>-49.713959</td>\n      <td>0</td>\n      <td>0.2640</td>\n      <td>0.2354</td>\n      <td>0.0109</td>\n      <td>40.3542</td>\n      <td>0.004</td>\n      <td>15</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>5954</th>\n      <td>87945160</td>\n      <td>235.195312</td>\n      <td>-20.264481</td>\n      <td>348.266405</td>\n      <td>27.337784</td>\n      <td>0</td>\n      <td>0.1965</td>\n      <td>0.4035</td>\n      <td>0.3118</td>\n      <td>41.7071</td>\n      <td>0.143</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6210</th>\n      <td>93834540</td>\n      <td>80.419357</td>\n      <td>-61.378410</td>\n      <td>270.539103</td>\n      <td>-34.263476</td>\n      <td>0</td>\n      <td>0.3235</td>\n      <td>0.3270</td>\n      <td>0.0168</td>\n      <td>41.1721</td>\n      <td>0.030</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3120</th>\n      <td>22540174</td>\n      <td>359.472656</td>\n      <td>-25.944481</td>\n      <td>35.549644</td>\n      <td>-77.970912</td>\n      <td>0</td>\n      <td>0.2423</td>\n      <td>0.2433</td>\n      <td>0.0078</td>\n      <td>40.4348</td>\n      <td>0.017</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6093</th>\n      <td>91022592</td>\n      <td>162.773438</td>\n      <td>-40.033035</td>\n      <td>279.236008</td>\n      <td>17.237327</td>\n      <td>0</td>\n      <td>0.5515</td>\n      <td>0.5449</td>\n      <td>0.0957</td>\n      <td>42.4861</td>\n      <td>0.097</td>\n      <td>15</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>6080</th>\n      <td>90750519</td>\n      <td>130.781250</td>\n      <td>-1.044512</td>\n      <td>227.507062</td>\n      <td>24.102441</td>\n      <td>0</td>\n      <td>0.1979</td>\n      <td>0.1912</td>\n      <td>0.0115</td>\n      <td>39.8475</td>\n      <td>0.027</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3728</th>\n      <td>37077473</td>\n      <td>132.714844</td>\n      <td>-0.298417</td>\n      <td>227.885162</td>\n      <td>26.146386</td>\n      <td>0</td>\n      <td>0.2204</td>\n      <td>0.2405</td>\n      <td>0.0270</td>\n      <td>40.4064</td>\n      <td>0.026</td>\n      <td>15</td>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\n<p>99 rows Ã— 13 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ef8d5c83eb22bbd612b9414631130707b4e4a2f"
      },
      "cell_type": "code",
      "source": "df_meta_test.loc[(df_meta_test.target == df_meta_test.pred_label) & (df_meta_test.target != 16) & (df_meta_test.target != 92) & (df_meta_test.target != 88)]",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "      object_id          ra       decl     ...      mwebv  target  pred_label\n1305     213707  150.468750   1.641510     ...      0.017      90          90\n1619     262311  150.292969   2.686724     ...      0.016      90          90\n5059   67875060  250.312500 -23.155577     ...      0.456      15          15\n5639   81144750  223.528030 -50.091457     ...      0.272      65          65\n6246   94529982  351.309021 -46.375080     ...      0.006      15          15\n155       26660  347.846710 -64.760857     ...      0.019      65          65\n1085     183932  150.996094   4.181528     ...      0.015      62          62\n5587   79914128  106.523438 -27.784405     ...      0.139      65          65\n7141  114997468   77.167969 -21.381943     ...      0.030      90          90\n5864   86230042  314.121094 -27.279613     ...      0.073      65          65\n2842   17071074  147.289154 -43.207821     ...      0.262       6           6\n6438   99179619   55.898438 -10.958863     ...      0.049      15          15\n2118    1107992  310.078125 -20.902014     ...      0.035      42          42\n604      102745  349.615387 -63.636005     ...      0.018      90          90\n4681   59194660   46.706161 -50.674160     ...      0.017      65          65\n3462   30576273   73.476562 -31.213543     ...      0.016      42          42\n1922     312334   52.910156 -25.944481     ...      0.010      42          42\n2835   16829513    9.667969 -14.785930     ...      0.015      62          62\n4797   61803685    1.757812 -10.958863     ...      0.032      42          42\n1719     278959   52.910156 -27.953188     ...      0.007      67          67\n5879   86487763  232.382812  -3.134927     ...      0.159       6           6\n7490  122963326   89.038460 -46.178181     ...      0.039      64          64\n2312    5345351  351.738281 -39.065495     ...      0.016      65          65\n3365   28492953   72.773438 -22.185358     ...      0.038      15          15\n3757   37796269  318.691406 -11.415158     ...      0.034      90          90\n3452   30415620  332.929688 -11.262980     ...      0.032      90          90\n1702     275093   33.574219  -5.079716     ...      0.016      90          90\n298       53025  358.636353 -46.768478     ...      0.008      65          65\n3785   38249774  204.786102 -55.299438     ...      0.522      42          42\n4847   62987535   40.429688 -13.401351     ...      0.022      42          42\n...         ...         ...        ...     ...        ...     ...         ...\n4041   44088638   39.902344  -0.298417     ...      0.021      90          90\n44         8688   32.695312  -4.929937     ...      0.018      65          65\n7252  117495237   81.738281 -25.282602     ...      0.053      65          65\n6402   98299516  225.351562   1.044512     ...      0.038      42          42\n337       59163  150.644531   3.583322     ...      0.018      65          65\n1387     226855   53.085938 -27.111860     ...      0.007      65          65\n4985   66252300   30.410156  -5.979157     ...      0.021      90          90\n3609   34260744  129.199219   1.193748     ...      0.040      42          42\n51         9543  352.132874 -63.636005     ...      0.021      65          65\n1259     207496   35.683594  -5.379379     ...      0.020      42          42\n84        14674   33.750000  -4.630479     ...      0.019      90          90\n1453     236647   33.574219  -6.579593     ...      0.021      90          90\n5801   84773554   82.968750  -4.331149     ...      0.386      62          62\n3733   37170843  315.853088 -50.674160     ...      0.025      42          42\n2041     331371  148.710938   2.836105     ...      0.031      65          65\n6043   90098804   31.464844 -21.702768     ...      0.012      90          90\n870      147571    0.190678 -45.783966     ...      0.005      90          90\n268       47148   53.085938 -27.784405     ...      0.007      90          90\n6983  111353970   12.480469 -12.024699     ...      0.021      67          67\n1269     208803  351.953644 -62.132156     ...      0.019      67          67\n4020   43479814  227.988281 -41.810314     ...      0.083      65          65\n5634   81042522  118.125000   1.044512     ...      0.033      42          42\n3618   34518664   29.423077 -46.178181     ...      0.018      62          62\n8          1920  149.414062   3.433834     ...      0.027      90          90\n6080   90750519  130.781250  -1.044512     ...      0.027      15          15\n399       69767    1.694561 -45.191612     ...      0.011      62          62\n4539   56002762  358.417969 -35.685333     ...      0.011      42          42\n2724   14473678  326.965332 -57.970295     ...      0.023      42          42\n6350   97111379  134.648438  -7.030393     ...      0.031      90          90\n3614   34401716  116.015625  -1.641510     ...      0.065      65          65\n\n[586 rows x 13 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_id</th>\n      <th>ra</th>\n      <th>decl</th>\n      <th>gal_l</th>\n      <th>gal_b</th>\n      <th>ddf</th>\n      <th>hostgal_specz</th>\n      <th>hostgal_photoz</th>\n      <th>hostgal_photoz_err</th>\n      <th>distmod</th>\n      <th>mwebv</th>\n      <th>target</th>\n      <th>pred_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1305</th>\n      <td>213707</td>\n      <td>150.468750</td>\n      <td>1.641510</td>\n      <td>237.714575</td>\n      <td>42.075234</td>\n      <td>1</td>\n      <td>0.2881</td>\n      <td>0.5509</td>\n      <td>0.1365</td>\n      <td>42.5147</td>\n      <td>0.017</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1619</th>\n      <td>262311</td>\n      <td>150.292969</td>\n      <td>2.686724</td>\n      <td>236.427488</td>\n      <td>42.541447</td>\n      <td>1</td>\n      <td>0.3915</td>\n      <td>0.4917</td>\n      <td>0.5289</td>\n      <td>42.2178</td>\n      <td>0.016</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>5059</th>\n      <td>67875060</td>\n      <td>250.312500</td>\n      <td>-23.155577</td>\n      <td>356.335534</td>\n      <td>15.125077</td>\n      <td>0</td>\n      <td>0.1537</td>\n      <td>0.1746</td>\n      <td>0.0176</td>\n      <td>39.6289</td>\n      <td>0.456</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5639</th>\n      <td>81144750</td>\n      <td>223.528030</td>\n      <td>-50.091457</td>\n      <td>322.234822</td>\n      <td>8.126019</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.272</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>6246</th>\n      <td>94529982</td>\n      <td>351.309021</td>\n      <td>-46.375080</td>\n      <td>338.712954</td>\n      <td>-64.313996</td>\n      <td>0</td>\n      <td>0.1890</td>\n      <td>0.2642</td>\n      <td>0.1869</td>\n      <td>40.6391</td>\n      <td>0.006</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>26660</td>\n      <td>347.846710</td>\n      <td>-64.760857</td>\n      <td>318.929827</td>\n      <td>-49.143596</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.019</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>1085</th>\n      <td>183932</td>\n      <td>150.996094</td>\n      <td>4.181528</td>\n      <td>235.291975</td>\n      <td>43.970869</td>\n      <td>1</td>\n      <td>0.1132</td>\n      <td>0.1289</td>\n      <td>0.0169</td>\n      <td>38.9073</td>\n      <td>0.015</td>\n      <td>62</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>5587</th>\n      <td>79914128</td>\n      <td>106.523438</td>\n      <td>-27.784405</td>\n      <td>239.455137</td>\n      <td>-9.336810</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.139</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7141</th>\n      <td>114997468</td>\n      <td>77.167969</td>\n      <td>-21.381943</td>\n      <td>222.430628</td>\n      <td>-31.703263</td>\n      <td>0</td>\n      <td>0.2809</td>\n      <td>0.2913</td>\n      <td>0.0080</td>\n      <td>40.8822</td>\n      <td>0.030</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>5864</th>\n      <td>86230042</td>\n      <td>314.121094</td>\n      <td>-27.279613</td>\n      <td>18.352990</td>\n      <td>-38.278270</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.073</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>2842</th>\n      <td>17071074</td>\n      <td>147.289154</td>\n      <td>-43.207821</td>\n      <td>271.208515</td>\n      <td>8.158789</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.262</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6438</th>\n      <td>99179619</td>\n      <td>55.898438</td>\n      <td>-10.958863</td>\n      <td>199.663279</td>\n      <td>-46.494255</td>\n      <td>0</td>\n      <td>0.1689</td>\n      <td>0.1972</td>\n      <td>0.0146</td>\n      <td>39.9226</td>\n      <td>0.049</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2118</th>\n      <td>1107992</td>\n      <td>310.078125</td>\n      <td>-20.902014</td>\n      <td>24.531144</td>\n      <td>-32.885543</td>\n      <td>0</td>\n      <td>0.1426</td>\n      <td>0.0502</td>\n      <td>0.0372</td>\n      <td>36.7429</td>\n      <td>0.035</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>604</th>\n      <td>102745</td>\n      <td>349.615387</td>\n      <td>-63.636005</td>\n      <td>318.927246</td>\n      <td>-50.506542</td>\n      <td>1</td>\n      <td>0.2460</td>\n      <td>0.2333</td>\n      <td>1.0359</td>\n      <td>40.3315</td>\n      <td>0.018</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>4681</th>\n      <td>59194660</td>\n      <td>46.706161</td>\n      <td>-50.674160</td>\n      <td>264.726359</td>\n      <td>-55.200853</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.017</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3462</th>\n      <td>30576273</td>\n      <td>73.476562</td>\n      <td>-31.213543</td>\n      <td>232.839472</td>\n      <td>-37.572167</td>\n      <td>0</td>\n      <td>0.1672</td>\n      <td>0.1890</td>\n      <td>0.0270</td>\n      <td>39.8190</td>\n      <td>0.016</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>1922</th>\n      <td>312334</td>\n      <td>52.910156</td>\n      <td>-25.944481</td>\n      <td>220.366350</td>\n      <td>-54.301439</td>\n      <td>1</td>\n      <td>0.3442</td>\n      <td>0.3728</td>\n      <td>0.0285</td>\n      <td>41.5044</td>\n      <td>0.010</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>2835</th>\n      <td>16829513</td>\n      <td>9.667969</td>\n      <td>-14.785930</td>\n      <td>108.753304</td>\n      <td>-77.304892</td>\n      <td>0</td>\n      <td>0.2108</td>\n      <td>2.3247</td>\n      <td>0.6188</td>\n      <td>46.3560</td>\n      <td>0.015</td>\n      <td>62</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>4797</th>\n      <td>61803685</td>\n      <td>1.757812</td>\n      <td>-10.958863</td>\n      <td>87.944383</td>\n      <td>-70.750522</td>\n      <td>0</td>\n      <td>0.1712</td>\n      <td>0.1538</td>\n      <td>0.0186</td>\n      <td>39.3249</td>\n      <td>0.032</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>1719</th>\n      <td>278959</td>\n      <td>52.910156</td>\n      <td>-27.953188</td>\n      <td>223.774083</td>\n      <td>-54.639214</td>\n      <td>1</td>\n      <td>0.2475</td>\n      <td>0.2452</td>\n      <td>0.0916</td>\n      <td>40.4542</td>\n      <td>0.007</td>\n      <td>67</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>5879</th>\n      <td>86487763</td>\n      <td>232.382812</td>\n      <td>-3.134927</td>\n      <td>0.750196</td>\n      <td>41.341230</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.159</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7490</th>\n      <td>122963326</td>\n      <td>89.038460</td>\n      <td>-46.178181</td>\n      <td>253.057349</td>\n      <td>-28.437839</td>\n      <td>0</td>\n      <td>0.0814</td>\n      <td>2.3500</td>\n      <td>0.7722</td>\n      <td>46.3846</td>\n      <td>0.039</td>\n      <td>64</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>2312</th>\n      <td>5345351</td>\n      <td>351.738281</td>\n      <td>-39.065495</td>\n      <td>353.432849</td>\n      <td>-68.741311</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.016</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3365</th>\n      <td>28492953</td>\n      <td>72.773438</td>\n      <td>-22.185358</td>\n      <td>221.757534</td>\n      <td>-35.824363</td>\n      <td>0</td>\n      <td>0.0340</td>\n      <td>0.0251</td>\n      <td>0.0174</td>\n      <td>35.1955</td>\n      <td>0.038</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3757</th>\n      <td>37796269</td>\n      <td>318.691406</td>\n      <td>-11.415158</td>\n      <td>39.073587</td>\n      <td>-36.937837</td>\n      <td>0</td>\n      <td>0.4180</td>\n      <td>0.4244</td>\n      <td>0.0195</td>\n      <td>41.8367</td>\n      <td>0.034</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>3452</th>\n      <td>30415620</td>\n      <td>332.929688</td>\n      <td>-11.262980</td>\n      <td>47.968629</td>\n      <td>-49.322169</td>\n      <td>0</td>\n      <td>0.3825</td>\n      <td>0.3657</td>\n      <td>0.4350</td>\n      <td>41.4553</td>\n      <td>0.032</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1702</th>\n      <td>275093</td>\n      <td>33.574219</td>\n      <td>-5.079716</td>\n      <td>168.448505</td>\n      <td>-60.407218</td>\n      <td>1</td>\n      <td>0.2651</td>\n      <td>0.2712</td>\n      <td>0.7636</td>\n      <td>40.7037</td>\n      <td>0.016</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>53025</td>\n      <td>358.636353</td>\n      <td>-46.768478</td>\n      <td>328.890146</td>\n      <td>-67.388837</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.008</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3785</th>\n      <td>38249774</td>\n      <td>204.786102</td>\n      <td>-55.299438</td>\n      <td>309.738437</td>\n      <td>6.940590</td>\n      <td>0</td>\n      <td>0.0474</td>\n      <td>0.0811</td>\n      <td>0.0102</td>\n      <td>37.8336</td>\n      <td>0.522</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>4847</th>\n      <td>62987535</td>\n      <td>40.429688</td>\n      <td>-13.401351</td>\n      <td>190.384983</td>\n      <td>-60.824022</td>\n      <td>0</td>\n      <td>0.1281</td>\n      <td>0.1595</td>\n      <td>0.2724</td>\n      <td>39.4120</td>\n      <td>0.022</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4041</th>\n      <td>44088638</td>\n      <td>39.902344</td>\n      <td>-0.298417</td>\n      <td>171.485154</td>\n      <td>-52.659679</td>\n      <td>0</td>\n      <td>0.3091</td>\n      <td>0.2881</td>\n      <td>0.2028</td>\n      <td>40.8544</td>\n      <td>0.021</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>8688</td>\n      <td>32.695312</td>\n      <td>-4.929937</td>\n      <td>166.868469</td>\n      <td>-60.841230</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.018</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>7252</th>\n      <td>117495237</td>\n      <td>81.738281</td>\n      <td>-25.282602</td>\n      <td>228.281287</td>\n      <td>-28.994390</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.053</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>6402</th>\n      <td>98299516</td>\n      <td>225.351562</td>\n      <td>1.044512</td>\n      <td>358.491039</td>\n      <td>49.363964</td>\n      <td>0</td>\n      <td>0.1393</td>\n      <td>0.1433</td>\n      <td>0.0222</td>\n      <td>39.1575</td>\n      <td>0.038</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>59163</td>\n      <td>150.644531</td>\n      <td>3.583322</td>\n      <td>235.698235</td>\n      <td>43.342784</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.018</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>1387</th>\n      <td>226855</td>\n      <td>53.085938</td>\n      <td>-27.111860</td>\n      <td>222.384291</td>\n      <td>-54.355086</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.007</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>4985</th>\n      <td>66252300</td>\n      <td>30.410156</td>\n      <td>-5.979157</td>\n      <td>164.373226</td>\n      <td>-63.054972</td>\n      <td>0</td>\n      <td>0.3044</td>\n      <td>0.3059</td>\n      <td>0.0490</td>\n      <td>41.0043</td>\n      <td>0.021</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>3609</th>\n      <td>34260744</td>\n      <td>129.199219</td>\n      <td>1.193748</td>\n      <td>224.520406</td>\n      <td>23.846977</td>\n      <td>0</td>\n      <td>0.0459</td>\n      <td>0.0794</td>\n      <td>0.0276</td>\n      <td>37.7830</td>\n      <td>0.040</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>9543</td>\n      <td>352.132874</td>\n      <td>-63.636005</td>\n      <td>317.424173</td>\n      <td>-51.095855</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.021</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>1259</th>\n      <td>207496</td>\n      <td>35.683594</td>\n      <td>-5.379379</td>\n      <td>171.992947</td>\n      <td>-59.253501</td>\n      <td>1</td>\n      <td>0.2339</td>\n      <td>0.2544</td>\n      <td>0.1653</td>\n      <td>40.5451</td>\n      <td>0.020</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>14674</td>\n      <td>33.750000</td>\n      <td>-4.630479</td>\n      <td>168.146242</td>\n      <td>-59.949072</td>\n      <td>1</td>\n      <td>0.2012</td>\n      <td>0.0567</td>\n      <td>0.4176</td>\n      <td>37.0171</td>\n      <td>0.019</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1453</th>\n      <td>236647</td>\n      <td>33.574219</td>\n      <td>-6.579593</td>\n      <td>170.455585</td>\n      <td>-61.548219</td>\n      <td>1</td>\n      <td>0.1436</td>\n      <td>0.1712</td>\n      <td>0.5904</td>\n      <td>39.5819</td>\n      <td>0.021</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>5801</th>\n      <td>84773554</td>\n      <td>82.968750</td>\n      <td>-4.331149</td>\n      <td>207.601719</td>\n      <td>-19.655860</td>\n      <td>0</td>\n      <td>0.1145</td>\n      <td>0.1241</td>\n      <td>1.0644</td>\n      <td>38.8190</td>\n      <td>0.386</td>\n      <td>62</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>3733</th>\n      <td>37170843</td>\n      <td>315.853088</td>\n      <td>-50.674160</td>\n      <td>347.955238</td>\n      <td>-41.289082</td>\n      <td>0</td>\n      <td>0.1212</td>\n      <td>2.5818</td>\n      <td>1.2023</td>\n      <td>46.6326</td>\n      <td>0.025</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>331371</td>\n      <td>148.710938</td>\n      <td>2.836105</td>\n      <td>235.050801</td>\n      <td>41.328739</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.031</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>6043</th>\n      <td>90098804</td>\n      <td>31.464844</td>\n      <td>-21.702768</td>\n      <td>199.205007</td>\n      <td>-72.232934</td>\n      <td>0</td>\n      <td>0.6035</td>\n      <td>0.6225</td>\n      <td>0.0086</td>\n      <td>42.8362</td>\n      <td>0.012</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>870</th>\n      <td>147571</td>\n      <td>0.190678</td>\n      <td>-45.783966</td>\n      <td>327.956322</td>\n      <td>-68.803772</td>\n      <td>1</td>\n      <td>0.1885</td>\n      <td>0.2575</td>\n      <td>0.1606</td>\n      <td>40.5748</td>\n      <td>0.005</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>47148</td>\n      <td>53.085938</td>\n      <td>-27.784405</td>\n      <td>223.525509</td>\n      <td>-54.460748</td>\n      <td>1</td>\n      <td>0.3153</td>\n      <td>0.3234</td>\n      <td>0.0167</td>\n      <td>41.1439</td>\n      <td>0.007</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>6983</th>\n      <td>111353970</td>\n      <td>12.480469</td>\n      <td>-12.024699</td>\n      <td>121.509522</td>\n      <td>-74.892259</td>\n      <td>0</td>\n      <td>0.2350</td>\n      <td>0.1319</td>\n      <td>0.0788</td>\n      <td>38.9626</td>\n      <td>0.021</td>\n      <td>67</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>1269</th>\n      <td>208803</td>\n      <td>351.953644</td>\n      <td>-62.132156</td>\n      <td>318.777388</td>\n      <td>-52.347124</td>\n      <td>1</td>\n      <td>0.2092</td>\n      <td>0.1875</td>\n      <td>0.0194</td>\n      <td>39.8002</td>\n      <td>0.019</td>\n      <td>67</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>4020</th>\n      <td>43479814</td>\n      <td>227.988281</td>\n      <td>-41.810314</td>\n      <td>329.140468</td>\n      <td>13.800687</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.083</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>5634</th>\n      <td>81042522</td>\n      <td>118.125000</td>\n      <td>1.044512</td>\n      <td>219.074924</td>\n      <td>14.040076</td>\n      <td>0</td>\n      <td>0.1250</td>\n      <td>0.1236</td>\n      <td>0.1995</td>\n      <td>38.8089</td>\n      <td>0.033</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>3618</th>\n      <td>34518664</td>\n      <td>29.423077</td>\n      <td>-46.178181</td>\n      <td>272.756620</td>\n      <td>-66.876834</td>\n      <td>0</td>\n      <td>0.1496</td>\n      <td>0.1328</td>\n      <td>0.0205</td>\n      <td>38.9775</td>\n      <td>0.018</td>\n      <td>62</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1920</td>\n      <td>149.414062</td>\n      <td>3.433834</td>\n      <td>234.919132</td>\n      <td>42.245550</td>\n      <td>1</td>\n      <td>0.3088</td>\n      <td>0.3229</td>\n      <td>0.3360</td>\n      <td>41.1401</td>\n      <td>0.027</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>6080</th>\n      <td>90750519</td>\n      <td>130.781250</td>\n      <td>-1.044512</td>\n      <td>227.507062</td>\n      <td>24.102441</td>\n      <td>0</td>\n      <td>0.1979</td>\n      <td>0.1912</td>\n      <td>0.0115</td>\n      <td>39.8475</td>\n      <td>0.027</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>69767</td>\n      <td>1.694561</td>\n      <td>-45.191612</td>\n      <td>326.278557</td>\n      <td>-69.858253</td>\n      <td>1</td>\n      <td>0.1603</td>\n      <td>2.4521</td>\n      <td>1.2066</td>\n      <td>46.4968</td>\n      <td>0.011</td>\n      <td>62</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>4539</th>\n      <td>56002762</td>\n      <td>358.417969</td>\n      <td>-35.685333</td>\n      <td>354.541149</td>\n      <td>-75.022451</td>\n      <td>0</td>\n      <td>0.0917</td>\n      <td>0.0957</td>\n      <td>0.0123</td>\n      <td>38.2146</td>\n      <td>0.011</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>2724</th>\n      <td>14473678</td>\n      <td>326.965332</td>\n      <td>-57.970295</td>\n      <td>335.942096</td>\n      <td>-45.650202</td>\n      <td>0</td>\n      <td>0.1744</td>\n      <td>0.4485</td>\n      <td>0.3010</td>\n      <td>41.9793</td>\n      <td>0.023</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>6350</th>\n      <td>97111379</td>\n      <td>134.648438</td>\n      <td>-7.030393</td>\n      <td>235.310162</td>\n      <td>24.173623</td>\n      <td>0</td>\n      <td>0.3369</td>\n      <td>0.4053</td>\n      <td>0.5045</td>\n      <td>41.7184</td>\n      <td>0.031</td>\n      <td>90</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>3614</th>\n      <td>34401716</td>\n      <td>116.015625</td>\n      <td>-1.641510</td>\n      <td>220.495095</td>\n      <td>10.919005</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.065</td>\n      <td>65</td>\n      <td>65</td>\n    </tr>\n  </tbody>\n</table>\n<p>586 rows Ã— 13 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0d86dfabe82bcfa6ab7d8fba0d6f060cda44517f"
      },
      "cell_type": "markdown",
      "source": "Just eyeballing the predictions, it doesn't seem like there's any particular class for which its doing good/bad, its just doing decent on average, which is perhaps an effect of the loss function. More investigation is needed here.\n\nFor now, we'll save our model so we can reuse it later."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04b7d7d0da3e2be87fd4fc9c43a06f3e1da9c2a9"
      },
      "cell_type": "code",
      "source": "time_stamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\nsave_file = 'model_{}.h5'.format(time_stamp)\nmodel.save(save_file)",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b314e9a73ef8702cea8c52a3c168e7237fd9fe9e"
      },
      "cell_type": "markdown",
      "source": "That's all for now! Feedback appreciated!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2a1cea478046e54d9bcb0f5cd8f5f415d105827"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}